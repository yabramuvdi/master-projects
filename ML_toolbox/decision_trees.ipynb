{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Using Decision Trees for Predicting Real Estate Valuation\n",
    "\n",
    "### ***Yabra Muvdi***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Home valuation is key in real estate industry, and also the basis for mortgages in credit sector. Here we have to predict the estimated value of a property. The data consists of a list of features plus the resulting $parcelvalue$, described in *Case_data_dictionary.xlsx* file. Each row corresponds to a particular home valuation, and $transactiondate$ is the date when the property was effectively sold. Properties are defined by $lotid$, but be aware that one property can be sold more than once (it's not the usual case). Also notice that some features are sometime empty, your model has to deal with it. Note that you don't have to use $totaltaxvalue$, $buildvalue$ or $landvalue$, because they are closely correlated with the final value to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read and explore data, report missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data\n",
    "data = pd.read_csv('Regression_Supervised_Train_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24755, 48)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Understand the size of the data set\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lotid</th>\n",
       "      <th>logerror</th>\n",
       "      <th>aircond</th>\n",
       "      <th>style</th>\n",
       "      <th>basement</th>\n",
       "      <th>numbath</th>\n",
       "      <th>numbedroom</th>\n",
       "      <th>classbuild</th>\n",
       "      <th>qualitybuild</th>\n",
       "      <th>decktype</th>\n",
       "      <th>...</th>\n",
       "      <th>unitnum</th>\n",
       "      <th>year</th>\n",
       "      <th>numstories</th>\n",
       "      <th>buildvalue</th>\n",
       "      <th>parcelvalue</th>\n",
       "      <th>taxyear</th>\n",
       "      <th>landvalue</th>\n",
       "      <th>totaltaxvalue</th>\n",
       "      <th>taxdelinquencyyear</th>\n",
       "      <th>mypointer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>2.475500e+04</td>\n",
       "      <td>1763.000000</td>\n",
       "      <td>14464.000000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>14654.000000</td>\n",
       "      <td>712.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14761.000000</td>\n",
       "      <td>24743.000000</td>\n",
       "      <td>9634.000000</td>\n",
       "      <td>2.475200e+04</td>\n",
       "      <td>2.475500e+04</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>2.475500e+04</td>\n",
       "      <td>24754.000000</td>\n",
       "      <td>886.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>1.335009e+07</td>\n",
       "      <td>0.012562</td>\n",
       "      <td>2.107232</td>\n",
       "      <td>7.141631</td>\n",
       "      <td>636.839286</td>\n",
       "      <td>2.603514</td>\n",
       "      <td>3.297637</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>5.640235</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006300</td>\n",
       "      <td>1972.886473</td>\n",
       "      <td>1.540378</td>\n",
       "      <td>2.382736e+05</td>\n",
       "      <td>5.430185e+05</td>\n",
       "      <td>2015.281761</td>\n",
       "      <td>3.047738e+05</td>\n",
       "      <td>6665.055184</td>\n",
       "      <td>13.969526</td>\n",
       "      <td>17672.183559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>3.444823e+06</td>\n",
       "      <td>0.129091</td>\n",
       "      <td>3.384228</td>\n",
       "      <td>2.051415</td>\n",
       "      <td>488.847319</td>\n",
       "      <td>1.045068</td>\n",
       "      <td>1.072567</td>\n",
       "      <td>0.383482</td>\n",
       "      <td>2.251645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123305</td>\n",
       "      <td>18.745019</td>\n",
       "      <td>0.531651</td>\n",
       "      <td>2.928330e+05</td>\n",
       "      <td>7.259855e+05</td>\n",
       "      <td>0.449867</td>\n",
       "      <td>4.930763e+05</td>\n",
       "      <td>8999.000816</td>\n",
       "      <td>1.523517</td>\n",
       "      <td>10215.193608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.071173e+07</td>\n",
       "      <td>-0.605100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1880.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.010000e+02</td>\n",
       "      <td>1.562300e+04</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>1.300000e+02</td>\n",
       "      <td>49.080000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>1.108076e+07</td>\n",
       "      <td>-0.026479</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1961.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.141252e+05</td>\n",
       "      <td>2.396560e+05</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>9.468850e+04</td>\n",
       "      <td>3042.225000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>8850.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1.212766e+07</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>556.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1974.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.725460e+05</td>\n",
       "      <td>3.842500e+05</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>1.936930e+05</td>\n",
       "      <td>4744.815000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>17660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>1.467828e+07</td>\n",
       "      <td>0.036462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>929.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1986.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.631998e+05</td>\n",
       "      <td>6.090000e+05</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>3.548305e+05</td>\n",
       "      <td>7387.355000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>26537.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1.629608e+08</td>\n",
       "      <td>2.489000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>2485.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.023363e+07</td>\n",
       "      <td>2.563981e+07</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>2.430208e+07</td>\n",
       "      <td>311386.080000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>35363.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              lotid     logerror       aircond       style     basement  \\\n",
       "count  2.475500e+04  1763.000000  14464.000000  233.000000    56.000000   \n",
       "mean   1.335009e+07     0.012562      2.107232    7.141631   636.839286   \n",
       "std    3.444823e+06     0.129091      3.384228    2.051415   488.847319   \n",
       "min    1.071173e+07    -0.605100      1.000000    2.000000    68.000000   \n",
       "25%    1.108076e+07    -0.026479      1.000000    7.000000   255.000000   \n",
       "50%    1.212766e+07     0.005000      1.000000    7.000000   556.000000   \n",
       "75%    1.467828e+07     0.036462      1.000000    7.000000   929.250000   \n",
       "max    1.629608e+08     2.489000     13.000000   21.000000  2485.000000   \n",
       "\n",
       "            numbath    numbedroom  classbuild  qualitybuild  decktype  ...  \\\n",
       "count  24755.000000  24755.000000   18.000000  14654.000000     712.0  ...   \n",
       "mean       2.603514      3.297637    3.833333      5.640235      66.0  ...   \n",
       "std        1.045068      1.072567    0.383482      2.251645       0.0  ...   \n",
       "min        0.000000      0.000000    3.000000      1.000000      66.0  ...   \n",
       "25%        2.000000      3.000000    4.000000      4.000000      66.0  ...   \n",
       "50%        2.500000      3.000000    4.000000      6.000000      66.0  ...   \n",
       "75%        3.000000      4.000000    4.000000      7.000000      66.0  ...   \n",
       "max       19.000000     11.000000    4.000000     12.000000      66.0  ...   \n",
       "\n",
       "            unitnum          year   numstories    buildvalue   parcelvalue  \\\n",
       "count  14761.000000  24743.000000  9634.000000  2.475200e+04  2.475500e+04   \n",
       "mean       1.006300   1972.886473     1.540378  2.382736e+05  5.430185e+05   \n",
       "std        0.123305     18.745019     0.531651  2.928330e+05  7.259855e+05   \n",
       "min        1.000000   1880.000000     1.000000  1.010000e+02  1.562300e+04   \n",
       "25%        1.000000   1961.000000     1.000000  1.141252e+05  2.396560e+05   \n",
       "50%        1.000000   1974.000000     2.000000  1.725460e+05  3.842500e+05   \n",
       "75%        1.000000   1986.000000     2.000000  2.631998e+05  6.090000e+05   \n",
       "max        9.000000   2016.000000     3.000000  1.023363e+07  2.563981e+07   \n",
       "\n",
       "            taxyear     landvalue  totaltaxvalue  taxdelinquencyyear  \\\n",
       "count  24755.000000  2.475500e+04   24754.000000          886.000000   \n",
       "mean    2015.281761  3.047738e+05    6665.055184           13.969526   \n",
       "std        0.449867  4.930763e+05    8999.000816            1.523517   \n",
       "min     2015.000000  1.300000e+02      49.080000            6.000000   \n",
       "25%     2015.000000  9.468850e+04    3042.225000           14.000000   \n",
       "50%     2015.000000  1.936930e+05    4744.815000           14.500000   \n",
       "75%     2016.000000  3.548305e+05    7387.355000           15.000000   \n",
       "max     2016.000000  2.430208e+07  311386.080000           15.000000   \n",
       "\n",
       "          mypointer  \n",
       "count  24755.000000  \n",
       "mean   17672.183559  \n",
       "std    10215.193608  \n",
       "min        1.000000  \n",
       "25%     8850.500000  \n",
       "50%    17660.000000  \n",
       "75%    26537.500000  \n",
       "max    35363.000000  \n",
       "\n",
       "[8 rows x 44 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initial overview of the data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Initial exploration of the variable of interest (*parcel value*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEVCAYAAADZ4CNuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3df5RV5X3v8fdHEGKvQUBHS0CFxEkayL1NYYIkuTHemBsHvHbsWpoMNw1guWX5q0m6mnszZtk0adIuvG1iwo0/Fo1ewTQisTVOqsRQbJo0F5QhMZrREEYhMoEKiiLEVjP4vX/s55Dt4cw5e4bZAwyf11pnnbOf5/k++9l7w3xn7/2cPYoIzMzMynLCkR6AmZmNbE40ZmZWKicaMzMrlRONmZmVyonGzMxK5URjZmalcqKxo46kbknnH+lxlE3S5yU9K+lfj/RYhoqkz0j62rHWt5XLicaGlaRtkt5fVbZI0r9UliNiRkR8t0E/UyWFpNElDbVUks4E/gSYHhG/OUR9hqRfStov6ReSvihp1FD0fbgkTZbUJ+lNNerukfTXR2JcNjycaMxqGIYEdjbwXETsGmhgg7H9dkScDFwA/HfgD4e4/0GJiF8A64CPVK1rIjAPWDHU67SjhxONHXXyZz2SZkvqkvSipGckfTE1+156fyH9Bv9OSSdIuk7SzyXtkrRS0im5fhekuuck/WnVej4j6W5JX5P0IrAorXu9pBck7ZT0FUljcv2FpKskbZG0T9LnJL0pxbwoaXW+fS7u/cBa4A1p7Len8t9Nlw1fkPRdSW+t2ieflPQo8MtGySAifgp8H3hbiu+Q9GQa5+OSfi/X9yJJP5B0g6Q9wGdS+R9KeiIXMzOVv0HS30naLWmrpI8WOKyQJZOPVJW1A90R8Vjq+8uStqf9t0nSe2p1JOl8Sb1VZfnjeUJum59Lx2JiwXHaEHOisaPdl4EvR8Q44E3A6lR+XnofHxEnR8R6YFF6/RfgjcDJwFcAJE0HbgI+DEwCTgEmV62rDbgbGA/8LXAA+GPgNOCdZGcJV1XFtAKzgDnA/wKWp3WcSfZDfn71BkXEPwJzgR1p7IskvRm4E/g40ATcD3yrKlHNBy5K29xXZ59Vtvc9wI9S0ZNp+RTgs8DXJE3KhZwLPAWcDvyFpMvIEs4CYBzwu8Bzkk4AvgX8mGz/XQB8XNKF9caT3AOcJuk/58o+AqzMLW8E3g5MBL4OfEPS6wr0Xe2jwCXAe4E3AM8DNw6iHxsKEeGXX8P2ArYB+4EXcq+XgH+pavP+9Pl7ZD8YT6vqZyoQwOhc2TrgqtzyW4BfAaOBTwN35up+A3glt57PAN9rMPaPA/fklgN4d255E/DJ3PIXgC/109f5QG9u+U+B1bnlE4BfAOfn9skfNBhfAC+S/VB9Evg8cEI/bR8B2tLnRcDTVfUPAB+rEXdujbbXAv83tx+/VmeMXwWWp8/N6RicXqf982SXA1/Td/X+q/Hv5gngglzdpMq/hSP9f+B4fPmMxo6ESyJifOXFoWcJeYuBNwM/lbRR0n+r0/YNwM9zyz8nSzJnpLrtlYqIeAl4rip+e35B0psl/YOkf02X0/6S7Owm75nc53+rsXxynfH2O/aIeDWNJ3/Wtb06qIaZETEhIt4UEdelfiqXDR9Jl+VeIDvbym9Ldd9nkiWrameTXfJ7IdfXp8j2cRErgA+ms5SPAN+O3H0qSX+SLtftTX2fwqH7vIizgXtyY3yC7Ay16DhtCDnR2FEtIrZExHyySzrXA3dL+g9kv71X20H2A6biLKCP7If/TmBKpULSScCp1aurWr4Z+CnQHNmlu08BGvzW1PWasUsS2Q/7X9QZXyGSzgb+BrgGODUl95/w2m2p7ns72aXKatuBrflfFCLi9RExr8hYIuL7ZAm+Dfh9cpfN0v2YTwIfBCakce6l9j7/JdlZaSV2FNklx/w451aN83WRTUqwYeZEY0c1Sb8vqSn9Zv5CKj4A7AZeJbsXU3En8MeSpkk6mewM5K7I7mfcDVws6V3pvsdnaZw0Xk92KWq/pN8CrhyyDTvUauAiSRdIOpFs6vPLwP8bgr4riXk3gKTLSZME6vgq8AlJs5Q5JyWsh4EX08SEkySNkvQ2Se8YwHhWkv3SMJ7sfk/F68l+MdgNjJb0abL7Q7X8DHidpIvS/roOGJurv4XsXtPZaZubJLUNYIw2hJxo7GjXCnRL2k82MaA9Iv49Xfr6C+AH6fLIHOA24A6y+zpbgX8H/gggIrrT51VkZzf7gF1kP8z78wmyKcL7yM4I7hr6zctExGay3/D/D/AscDFwcUS8MgR9P052v2g92dndfwR+0CDmG2T79+tk2/9NYGJEHEhjezvZPn6WLCmd0k9XtawkO9u8KyLy+/8BYA1ZEvk52fGrebkwIvaSXXL9KtlZ3y+B/Cy0LwOdwHck7QM2kN1fsiNAEf7DZ3b8SWc8L5BdFtt6pMdjNpL5jMaOG5IulvQb6R7PXwOPkc1UMrMSOdHY8aSN7Kb7DrKpte3hU3qz0hVKNJJaJW2W1COpo0a9JC1L9Y9WvkFcL1bSRElrlX2req2kCbm6a1P7zfkvgqUbk4+lumVpZg6SzpP0Q2XPUrq0xvjGKXv201eK7xobaSLif6TZR6dExAXpvoiZlaxhoknTBm8k+ybzdGB++tZx3lyy3xCbgSVk00IbxXYA6yKimeyLdh0pZjrZYylmkN0Ivkm/fjDgzan/yrpaU/nTZF86+3o/m/E54J8bbauZmQ29Ig/Pmw30RMRTAJJWkV2CeDzXpg1YmS5DbJA0Pj3eYmqd2Dayb/dC9iWu75LNoW8DVqXZKFsl9QCzJW0DxkX2qBEkrSR7xMSaiNiWyl6tHrykWWRf0vo20NJoY0877bSYOnVqgd1iZmYVmzZtejYimmrVFUk0k3ntFMNeDp0mWKvN5AaxZ0TEToCI2Cnp9FxfG2r09SteO32xUt6v9FymL5B9A/mCOu2WkJ0pcdZZZ9HV1VWvWzMzqyLp5/3VFblHU+tLbdU3UPtrUyS26PoG09dVwP0RUffRHRGxPCJaIqKlqalmQjYzs0EqckbTS/YojIopZLN2irQZUyf2GUmT0tnMJLIvz9Xrq5fcI0T6GUe1dwLvkXQV2TOnxkjaHxGHTGgwM7NyFDmj2Qg0p8d6jCG7Ud9Z1aYTWJBmn80B9qbLYvViO4GF6fNC4N5cebuksZKmkd30fzj1t0/SnDTbbEEupqaI+HBEnBURU8m+5b3SScbMbHg1PKOJiD5J15A9HmIUcFtEdEu6ItXfQva3M+YBPWSPfL+8XmzqeimwWtJislljl6WYbkmrySYM9AFXp8deQPasqduBk8geVbEGID1n6R5gAtnzrD4bETMGvVfMzGzI+BE0VVpaWsKTAczMBkbSpoioObPXTwYwM7NSOdGYmVmpnGjMzKxUTjRmZlaqIt+jsQGY2nFfzfJtSy8a5pGYmR0dfEZjZmalcqIxM7NSOdGYmVmpnGjMzKxUTjRmZlYqJxozMyuVE42ZmZXKicbMzErlRGNmZqVyojEzs1I50ZiZWamcaMzMrFRONGZmVionGjMzK5UTjZmZlcqJxszMSuVEY2ZmpSqUaCS1StosqUdSR416SVqW6h+VNLNRrKSJktZK2pLeJ+Tqrk3tN0u6MFc+S9JjqW6ZJKXy8yT9UFKfpEtz7d8uab2k7jSuDw18F5mZ2eFomGgkjQJuBOYC04H5kqZXNZsLNKfXEuDmArEdwLqIaAbWpWVSfTswA2gFbkr9kPpdkltXayp/GlgEfL1qXC8BCyKi0teXJI1vtM1mZjZ0ipzRzAZ6IuKpiHgFWAW0VbVpA1ZGZgMwXtKkBrFtwIr0eQVwSa58VUS8HBFbgR5gdupvXESsj4gAVlZiImJbRDwKvJofVET8LCK2pM87gF1AU4FtNjOzIVIk0UwGtueWe1NZkTb1Ys+IiJ0A6f30An31NhhHvyTNBsYAT9aoWyKpS1LX7t27i3ZpZmYFFEk0qlEWBdsUiS26vsH0lXWYnQ3dAVweEa9W10fE8ohoiYiWpiaf8JiZDaUiiaYXODO3PAXYUbBNvdhnUgKoJIJdBfqa0mAch5A0DrgPuC5d1jMzs2FUJNFsBJolTZM0huxGfWdVm05gQZp9NgfYmy6H1YvtBBamzwuBe3Pl7ZLGSppGdtP/4dTfPklz0myzBbmYmtI67yG7f/SNAttqZmZDbHSjBhHRJ+ka4AFgFHBbRHRLuiLV3wLcD8wju3H/EnB5vdjU9VJgtaTFZLPGLksx3ZJWA48DfcDVEXEgxVwJ3A6cBKxJLyS9gyyhTAAulvTZNNPsg8B5wKmSFqU+FkXEIwPdUWZmNjjKJnBZRUtLS3R1dQ06fmrHfTXLty29aNB9mpkd7SRtioiWWnV+MoCZmZXKicbMzErlRGNmZqVyojEzs1I50ZiZWamcaMzMrFRONGZmVionGjMzK5UTjZmZlcqJxszMSuVEY2ZmpXKiMTOzUjnRmJlZqZxozMysVE40ZmZWKicaMzMrlRONmZmVyonGzMxK5URjZmalcqIxM7NSOdGYmVmpnGjMzKxUhRKNpFZJmyX1SOqoUS9Jy1L9o5JmNoqVNFHSWklb0vuEXN21qf1mSRfmymdJeizVLZOkVH6epB9K6pN0adXYFqZ1bJG0cGC7x8zMDlfDRCNpFHAjMBeYDsyXNL2q2VygOb2WADcXiO0A1kVEM7AuLZPq24EZQCtwU+qH1O+S3LpaU/nTwCLg61Vjnwj8GXAuMBv4s3xCMzOz8hU5o5kN9ETEUxHxCrAKaKtq0wasjMwGYLykSQ1i24AV6fMK4JJc+aqIeDkitgI9wOzU37iIWB8RAaysxETEtoh4FHi1alwXAmsjYk9EPA+s5dfJyczMhkGRRDMZ2J5b7k1lRdrUiz0jInYCpPfTC/TV22Acgxk7kpZI6pLUtXv37gZdmpnZQBRJNKpRFgXbFIktur6h7Ou1BRHLI6IlIlqampoadGlmZgNRJNH0AmfmlqcAOwq2qRf7TLocRnrfVaCvKQ3GMZixm5lZiYokmo1As6RpksaQ3ajvrGrTCSxIs8/mAHvT5bB6sZ1AZRbYQuDeXHm7pLGSppHd9H849bdP0pw022xBLqY/DwAfkDQhTQL4QCozM7NhMrpRg4jok3QN2Q/oUcBtEdEt6YpUfwtwPzCP7Mb9S8Dl9WJT10uB1ZIWk80auyzFdEtaDTwO9AFXR8SBFHMlcDtwErAmvZD0DuAeYAJwsaTPRsSMiNgj6XNkCQ/gzyNizyD2k5mZDZKyCVxW0dLSEl1dXYOOn9pxX83ybUsvGnSfZmZHO0mbIqKlVp2fDGBmZqVyojEzs1I50ZiZWamcaMzMrFRONGZmVionGjMzK5UTjZmZlcqJxszMSuVEY2ZmpXKiMTOzUjnRmJlZqZxozMysVE40ZmZWKicaMzMrlRONmZmVyonGzMxK5URjZmalcqIxM7NSOdGYmVmpnGjMzKxUTjRmZlYqJxozMytVoUQjqVXSZkk9kjpq1EvSslT/qKSZjWIlTZS0VtKW9D4hV3dtar9Z0oW58lmSHkt1yyQplY+VdFcqf0jS1FzM/5bULemJfIyZmQ2PholG0ijgRmAuMB2YL2l6VbO5QHN6LQFuLhDbAayLiGZgXVom1bcDM4BW4KbUD6nfJbl1tabyxcDzEXEOcANwferrXcC7gf8EvA14B/DeAvvFzMyGSJEzmtlAT0Q8FRGvAKuAtqo2bcDKyGwAxkua1CC2DViRPq8ALsmVr4qIlyNiK9ADzE79jYuI9RERwMqqmEpfdwMXpDOXAF4HjAHGAicCzxTYZjMzGyJFEs1kYHtuuTeVFWlTL/aMiNgJkN5PL9BXbz99HYyJiD5gL3BqRKwH/gnYmV4PRMQT1RsoaYmkLkldu3fvrrELzMxssIokmlr3NKJgmyKxRddXr6+adZLOAd4KTCFLRu+TdN4hDSOWR0RLRLQ0NTU1GJ6ZmQ1EkUTTC5yZW54C7CjYpl7sM+lyGOl9V4G+pvTT18EYSaOBU4A9wO8BGyJif0TsB9YAcxpusZmZDZkiiWYj0CxpmqQxZDfqO6vadAIL0uyzOcDedDmsXmwnsDB9XgjcmytvTzPJppHd9H849bdP0px0/2VBVUylr0uBB9N9nKeB90oaLelEsokAh1w6MzOz8oxu1CAi+iRdAzwAjAJui4huSVek+luA+4F5ZDfuXwIurxebul4KrJa0mCwhXJZiuiWtBh4H+oCrI+JAirkSuB04iezsZE0qvxW4Q1IP2ZlMeyq/G3gf8BjZZbZvR8S3BrSHzMzssCj7xd8qWlpaoqura9DxUzvuq1m+belFg+7TzOxoJ2lTRLTUqvOTAczMrFRONGZmVionGjMzK5UTjZmZlcqJxszMSuVEY2ZmpXKiMTOzUjnRmJlZqZxozMysVE40ZmZWKicaMzMrlRONmZmVyonGzMxK5URjZmalcqIxM7NSOdGYmVmpnGjMzKxUTjRmZlYqJxozMyuVE42ZmZXKicbMzErlRGNmZqUqlGgktUraLKlHUkeNeklaluoflTSzUaykiZLWStqS3ifk6q5N7TdLujBXPkvSY6lumSSl8rGS7krlD0mamos5S9J3JD0h6fF8nZmZla9hopE0CrgRmAtMB+ZLml7VbC7QnF5LgJsLxHYA6yKiGViXlkn17cAMoBW4KfVD6ndJbl2tqXwx8HxEnAPcAFyfG9tK4K8i4q3AbGBXo202M7OhU+SMZjbQExFPRcQrwCqgrapNG7AyMhuA8ZImNYhtA1akzyuAS3LlqyLi5YjYCvQAs1N/4yJifUQEWQK5pEZfdwMXpLOs6cDoiFgLEBH7I+KlQnvGzMyGRJFEMxnYnlvuTWVF2tSLPSMidgKk99ML9NXbT18HYyKiD9gLnAq8GXhB0t9L+pGkv8qdHR0kaYmkLkldu3fvrrkTzMxscIokGtUoi4JtisQWXV+9vvqrGw28B/gE8A7gjcCiQxpGLI+IlohoaWpqajA8MzMbiCKJphc4M7c8BdhRsE292GfS5TDSe+XeSb2+pvTT18EYSaOBU4A9qfxH6dJdH/BNYCZmZjZsiiSajUCzpGmSxpDdqO+satMJLEj3ReYAe9PlsHqxncDC9HkhcG+uvD3NJJtGdtP/4dTfPklz0myzBVUxlb4uBR5M93E2AhMkVU5T3gc8XmCbzcxsiIxu1CAi+iRdAzwAjAJui4huSVek+luA+4F5ZDfuXwIurxebul4KrJa0GHgauCzFdEtaTZYQ+oCrI+JAirkSuB04CViTXgC3AndI6iE7k2lPfR2Q9AlgXUpOm4C/GfBeMjOzQVP2i79VtLS0RFdX16Djp3bcV7N829KLBt2nmdnRTtKmiGipVecnA5iZWamcaMzMrFRONGZmVionGjMzK5UTjZmZlcqJxszMSuVEY2ZmpXKiMTOzUjnRmJlZqZxozMysVE40ZmZWKicaMzMrlRONmZmVyonGzMxK5URjZmalcqIxM7NSOdGYmVmpnGjMzKxUTjRmZlaq0Ud6AMeLqR331SzftvSiYR6Jmdnw8hmNmZmVyonGzMxKVSjRSGqVtFlSj6SOGvWStCzVPyppZqNYSRMlrZW0Jb1PyNVdm9pvlnRhrnyWpMdS3TJJSuVjJd2Vyh+SNLVqfOMk/ULSVwayc8zM7PA1TDSSRgE3AnOB6cB8SdOrms0FmtNrCXBzgdgOYF1ENAPr0jKpvh2YAbQCN6V+SP0uya2rNZUvBp6PiHOAG4Drq8b3OeCfG22rmZkNvSJnNLOBnoh4KiJeAVYBbVVt2oCVkdkAjJc0qUFsG7AifV4BXJIrXxURL0fEVqAHmJ36GxcR6yMigJVVMZW+7gYuyJ3tzALOAL5TZIeYmdnQKpJoJgPbc8u9qaxIm3qxZ0TEToD0fnqBvnr76etgTET0AXuBUyWdAHwB+J/1NlDSEkldkrp2795dr6mZmQ1QkUSjGmVRsE2R2KLrq9dXf3VXAfdHxPYa9b9uGLE8IloioqWpqanB8MzMbCCKfI+mFzgztzwF2FGwzZg6sc9ImhQRO9NlsV0N+upNn2v1VYnplTQaOAXYA7wTeI+kq4CTgTGS9kfEIRMazMysHEXOaDYCzZKmSRpDdqO+s6pNJ7AgzT6bA+xNl8PqxXYCC9PnhcC9ufL2NJNsGtlN/4dTf/skzUn3XxZUxVT6uhR4MN0v+nBEnBURU4FPkN1HcpIxMxtGDc9oIqJP0jXAA8Ao4LaI6JZ0Raq/BbgfmEd24/4l4PJ6sanrpcBqSYuBp4HLUky3pNXA40AfcHVEHEgxVwK3AycBa9IL4FbgDkk9ZGcy7YPbHWZmNtSUTeCyipaWlujq6hp0fH+PmumPH0FjZiOBpE0R0VKrzk8GMDOzUjnRmJlZqZxozMysVE40ZmZWKicaMzMrlRONmZmVyonGzMxK5URjZmalcqIxM7NSOdGYmVmpnGjMzKxUTjRmZlYqJxozMyuVE42ZmZXKicbMzErlRGNmZqVyojEzs1I50ZiZWamcaMzMrFRONGZmVionGjMzK5UTjZmZlapQopHUKmmzpB5JHTXqJWlZqn9U0sxGsZImSloraUt6n5Cruza13yzpwlz5LEmPpbplkpTKx0q6K5U/JGlqKn+7pPWSutO4PjSYnWRmZoPXMNFIGgXcCMwFpgPzJU2vajYXaE6vJcDNBWI7gHUR0QysS8uk+nZgBtAK3JT6IfW7JLeu1lS+GHg+Is4BbgCuT+UvAQsiotLXlySNb7xbzMxsqBQ5o5kN9ETEUxHxCrAKaKtq0wasjMwGYLykSQ1i24AV6fMK4JJc+aqIeDkitgI9wOzU37iIWB8RAaysiqn0dTdwgSRFxM8iYgtAROwAdgFNRXaMmZkNjSKJZjKwPbfcm8qKtKkXe0ZE7ARI76cX6Ku3n74OxkREH7AXODU/QEmzgTHAk9UbKGmJpC5JXbt3766uNjOzw1Ak0ahGWRRsUyS26Prq9VV3Pels6A7g8oh49ZCGEcsjoiUiWpqafMJjZjaUiiSaXuDM3PIUYEfBNvVin0kJoJIIdhXoa0o/fR2MkTQaOAXYk5bHAfcB16XLemZmNoyKJJqNQLOkaZLGkN2o76xq0wksSLPP5gB70+WwerGdwML0eSFwb668Pc0km0Z20//h1N8+SXPSbLMFVTGVvi4FHoyISOu8h+z+0TeK7RIzMxtKoxs1iIg+SdcADwCjgNsiolvSFan+FuB+YB7ZjfuXgMvrxaaulwKrJS0GngYuSzHdklYDjwN9wNURcSDFXAncDpwErEkvgFuBOyT1kJ3JtKfyDwLnAadKWpTKFkXEI4X3kJmZHRZlE7isoqWlJbq6ugYdP7XjvgG137b0okGvy8zsaCFpU0S01KrzkwHMzKxUTjRmZlYqJxozMyuVE42ZmZXKicbMzErlRGNmZqVyojEzs1I50ZiZWamcaMzMrFRONGZmVionGjMzK1XDh2paufp7NpqfgWZmI4XPaMzMrFRONGZmVionGjMzK5UTjZmZlcqJxszMSuVEY2ZmpXKiMTOzUvl7NEcpf7/GzEYKn9GYmVmpnGjMzKxUhS6dSWoFvgyMAr4aEUur6pXq5wEvAYsi4of1YiVNBO4CpgLbgA9GxPOp7lpgMXAA+GhEPJDKZwG3AycB9wMfi4iQNBZYCcwCngM+FBHbUsxC4Lo01M9HxIrCe+co5EtqZnasaZhoJI0CbgT+K9ALbJTUGRGP55rNBZrT61zgZuDcBrEdwLqIWCqpIy1/UtJ0oB2YAbwB+EdJb46IA6nfJcAGskTTCqwhS0rPR8Q5ktqB64EPpWT2Z0ALEMCmtP7nB7vDjlZOQGZ2tCpyRjMb6ImIpwAkrQLagHyiaQNWRkQAGySNlzSJ7Gylv9g24PwUvwL4LvDJVL4qIl4GtkrqAWZL2gaMi4j1qa+VwCVkiaYN+Ezq627gK+ks60JgbUTsSTFryZLTncV2z7GvvwQ0lJzMzKyeIolmMrA9t9xLdtbSqM3kBrFnRMROgIjYKen0XF8bavT1q/S5uvw164+IPkl7gVPrjOs1JC0hO1MC2C9pc3Wbgk4Dnh1k7LHoNOBZXX+khzEsjstje6QHMUy8rUPj7P4qiiQa1SiLgm2KxBZdX72+Dmv9EbEcWN5gXA1J6oqIlsPt51hxPG3v8bStcHxtr7e1fEVmnfUCZ+aWpwA7CrapF/tMurxGet9VoK8p/fR1MEbSaOAUYE/BsZuZWYmKJJqNQLOkaZLGkN2o76xq0wksUGYOsDddFqsX2wksTJ8XAvfmytsljZU0jWyCwcOpv32S5qT7LwuqYip9XQo8mO4XPQB8QNIESROAD6QyMzMbJg0vnaV7HteQ/YAeBdwWEd2Srkj1t5DNAJsH9JBNb768XmzqeimwWtJi4GngshTTLWk12YSBPuDqNOMM4Ep+Pb15TXoB3ArckSYO7CFLaETEHkmfI0t4AH9emRhQksO+/HaMOZ6293jaVji+ttfbWjJlv/ibmZmVw08GMDOzUjnRmJlZqZxoBkFSq6TNknrSUw2q6yVpWap/VNLMIzHOoVBgW8+XtFfSI+n16SMxzqEg6TZJuyT9pJ/6EXNcodD2jqRje6akf5L0hKRuSR+r0WZEHN+C2zq8xzYi/BrAi2xSw5PAG4ExwI+B6VVt5pFNVBAwB3joSI+7xG09H/iHIz3WIdre84CZwE/6qR8Rx3UA2zuSju0kYGb6/HrgZyP4/22RbR3WY+szmoE7+EieiHgFqDxWJ+/gI3kiYgNQeSTPsabIto4YEfE9slmL/RkpxxUotL0jRkTsjPSg34jYBzzBoU8JGRHHt+C2DisnmoEr8libQo++OQYU3Y53SvqxpDWSZgzP0I6IkXJcB2LEHVtJU4HfAR6qqhpxx7fOtsIwHlv/hc2BO5xH8hxrimzHD4GzI2K/pHnAN8m+ZDsSjZTjWtSIO7aSTgb+Dvh4RLxYXV0j5Jg9vg22dViPrc9oBu5wHslzrGm4HRHxYkTsT5/vB06UdNrwDXFYjZTjWshIO7aSTiT7wfu3EfH3NZqMmOPbaFuH+7afsMYAAAGySURBVNg60Qzc4TyS51jTcFsl/WZ6JBCSZpP9m3pu2Ec6PEbKcS1kJB3btB23Ak9ExBf7aTYijm+RbR3uY+tLZwMUh/FInmNNwW29FLhSUh/wb0B7pGktxxpJd5LNxjlNUi/ZH807EUbWca0osL0j5tgC7wY+Ajwm6ZFU9ingLBhxx7fItg7rsfUjaMzMrFS+dGZmZqVyojEzs1I50ZiZWamcaMzMrFRONGZmx7lGD1itantD7mGcP5P0QsMYzzozMzu+SToP2E/2rLe3DSDuj4DfiYg/qNfOZzRmZse5Wg9YlfQmSd+WtEnS9yX9Vo3Q+cCdjfr3FzbNzKyW5cAVEbFF0rnATcD7KpWSzgamAQ826siJxszMXiM9kPNdwDfSk2oAxlY1awfujogDjfpzojEzs2onAC9ExNvrtGkHri7amZmZ2UHpzwpslXQZHPwz179dqZf0FmACsL5If040ZmbHufSA1fXAWyT1SloMfBhYLOnHQDev/eu684FVRR/E6enNZmZWKp/RmJlZqZxozMysVE40ZmZWKicaMzMrlRONmZmVyonGzMxK5URjZmal+v9btU2t0DxR5gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data.parcelvalue, density = True, bins = 50)\n",
    "plt.title('Histogram for Parcel Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this initial histogram we can see that we are dealing with highly skewed data. Prediction is going to do be difficult if we do not thing of a sound strategy to deal with this. A first approach could be to transform the variable with the $\\log$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(data.parcelvalue), density = True, bins = 50)\n",
    "plt.title('Histogram for the logarithm of the Parcel Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this simple transformation we now have a distribution of the parcel value that is not skewed and appears to be gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Exploration of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the pressence of missing values (as a percentage of all the observartions)\n",
    "missing = data.isna().sum()/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lotid                 0.000000\n",
       "logerror              0.928782\n",
       "transactiondate       0.928782\n",
       "aircond               0.415714\n",
       "style                 0.990588\n",
       "basement              0.997738\n",
       "numbath               0.000000\n",
       "numbedroom            0.000000\n",
       "classbuild            0.999273\n",
       "qualitybuild          0.408039\n",
       "decktype              0.971238\n",
       "finishedarea1st       0.751848\n",
       "finishedarea          0.002545\n",
       "perimeterarea         1.000000\n",
       "totalarea             0.997495\n",
       "finishedareaEntry     0.751848\n",
       "countycode            0.000000\n",
       "numfireplace          0.000000\n",
       "numfullbath           0.001454\n",
       "garagenum             0.596284\n",
       "garagearea            0.596284\n",
       "tubflag               0.949707\n",
       "heatingtype           0.362189\n",
       "latitude              0.000000\n",
       "longitude             0.000000\n",
       "lotarea               0.028641\n",
       "poolnum               0.486124\n",
       "poolarea              0.951283\n",
       "citycode              0.001252\n",
       "countycode2           0.000000\n",
       "neighborhoodcode      0.392971\n",
       "regioncode            0.000081\n",
       "roomnum               0.000000\n",
       "storytype             0.997738\n",
       "num34bath             0.797253\n",
       "material              0.989941\n",
       "unitnum               0.403716\n",
       "year                  0.000485\n",
       "numstories            0.610826\n",
       "fireplace             0.992406\n",
       "buildvalue            0.000121\n",
       "parcelvalue           0.000000\n",
       "taxyear               0.000000\n",
       "landvalue             0.000000\n",
       "totaltaxvalue         0.000040\n",
       "taxdelinquencyflag    0.964209\n",
       "taxdelinquencyyear    0.964209\n",
       "mypointer             0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build a function to make this output more understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_report(df, low=0.15, medium=0.35):\n",
    "    \"\"\"Function for creating a simple report on the presence of missing values \n",
    "    of a data set ussing the thresholds provided as parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    #create series with percentage of missing values per column\n",
    "    missing = df.isna().sum()/df.shape[0]\n",
    "    \n",
    "    #1. features with no missing values\n",
    "    no_nas = missing[missing == 0]\n",
    "    \n",
    "    print('\\n **************************************************************************************** \\n', \n",
    "          '                 ', 'Features with no missing values: ', len(no_nas), '                  ',\n",
    "          '\\n ****************************************************************************************')\n",
    "    \n",
    "    #2. features with 'low' number of missing values\n",
    "    low_nas = missing[(missing <= low) & (missing > 0)]\n",
    "    \n",
    "    print('\\n **************************************************************************************** \\n', \n",
    "          '      ', 'Features with ', low*100, '% or less missing values: ', len(low_nas), '        ',\n",
    "          '\\n ****************************************************************************************')\n",
    "    \n",
    "    #3. features with 'medium' number of missing values\n",
    "    medium_nas = missing[(missing <= medium) & (missing > low)]\n",
    "    \n",
    "    print('\\n **************************************************************************************** \\n', \n",
    "          ' ', 'Features with missing values between', low*100, '% and ', medium*100, '% : ', len(medium_nas),' ',\n",
    "          '\\n ****************************************************************************************')\n",
    "    \n",
    "    if len(medium_nas) > 0:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.barh(medium_nas.index, medium_nas)\n",
    "        plt.title('Features with medium number of missing values')\n",
    "        plt.show()\n",
    "    \n",
    "    #4. features with 'large' number of missing values\n",
    "    large_nas = missing[(missing >= medium)]\n",
    "    \n",
    "    print('\\n **************************************************************************************** \\n', \n",
    "          'WARNING: ', 'Features with ', medium*100, '% or more missing values: ', len(large_nas), '        ',\n",
    "          '\\n ****************************************************************************************')\n",
    "    \n",
    "    if len(large_nas) > 0:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.barh(large_nas.index, large_nas)\n",
    "        plt.title('Features with large number of missing values')\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **************************************************************************************** \n",
      "                   Features with no missing values:  13                    \n",
      " ****************************************************************************************\n",
      "\n",
      " **************************************************************************************** \n",
      "        Features with  15.0 % or less missing values:  8          \n",
      " ****************************************************************************************\n",
      "\n",
      " **************************************************************************************** \n",
      "   Features with missing values between 15.0 % and  35.0 % :  0   \n",
      " ****************************************************************************************\n",
      "\n",
      " **************************************************************************************** \n",
      " WARNING:  Features with  35.0 % or more missing values:  27          \n",
      " ****************************************************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "missing_report(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values are clearly an important issue that deserves consideration for this dataset. We will do this in the following step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Remove features with missing data, and then observations with missing data\n",
    "\n",
    "Dealing with missing data is difficult. It is not always clear when a feature has such a low amount of available data that maybe it is not worth considering. We will try an approach that uses different tools to deal with missing data hoping that it does a sensible job. The steps to clean the data will be the following:\n",
    "\n",
    "1. Remove the features with more than 80% of missing data. It seems that whatever we do with these features we would be introducing very noisy measures.\n",
    "2. Use sklearn IterativeImputer to impute the rest of the missing data.\n",
    "3. Test how results change if we use a different threshold for removing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1. : Handling features with a really high percentage of missing values (> 80%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets start by making a copy of the data\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try with 80%\n",
    "high_missing = df.columns[missing > 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(axis=1,labels = high_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24755, 33)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By dropping these features we have now reduce our dataset to 33 features. This is potentially good because we will be shortly introducing new varibles when creating dummies and categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2. Imputing missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to imputting missing data using the IterativeImputer from sklearn. However, I found that it could to interpret the results of the imputation since most of the variables are categorical and the imputer is going to give values really different from the categories 'allowed'. For this reason, I will pursue a simpler apporach using SimpleImputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly require this experimental feature\n",
    "#from sklearn.experimental import enable_iterative_imputer\n",
    "# now we can import normally from sklearn.impute\n",
    "#from sklearn.impute import IterativeImputer\n",
    "#imputer = IterativeImputer(initial_strategy = 'median', random_state = 92)\n",
    "#imputer\n",
    "#df_imp = imputer.fit_transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "              missing_values=nan, strategy='median', verbose=0)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "simp_imputer = SimpleImputer(strategy = 'median')\n",
    "simp_imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp = simp_imputer.fit_transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the imputer\n",
    "import pickle\n",
    "filename = 'simple_imputer.sav'\n",
    "pickle.dump(simp_imputer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24755, 33)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconvert the imputed data into a pandas DataFrame\n",
    "df_imp = pd.DataFrame(df_imp, columns = df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lotid</th>\n",
       "      <th>aircond</th>\n",
       "      <th>numbath</th>\n",
       "      <th>numbedroom</th>\n",
       "      <th>qualitybuild</th>\n",
       "      <th>finishedarea1st</th>\n",
       "      <th>finishedarea</th>\n",
       "      <th>finishedareaEntry</th>\n",
       "      <th>countycode</th>\n",
       "      <th>numfireplace</th>\n",
       "      <th>...</th>\n",
       "      <th>num34bath</th>\n",
       "      <th>unitnum</th>\n",
       "      <th>year</th>\n",
       "      <th>numstories</th>\n",
       "      <th>buildvalue</th>\n",
       "      <th>parcelvalue</th>\n",
       "      <th>taxyear</th>\n",
       "      <th>landvalue</th>\n",
       "      <th>totaltaxvalue</th>\n",
       "      <th>mypointer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17214744.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1255.0</td>\n",
       "      <td>1255.0</td>\n",
       "      <td>1255.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>103993.0</td>\n",
       "      <td>142212.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>38219.0</td>\n",
       "      <td>1715.08</td>\n",
       "      <td>25711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12018724.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1033.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129428.0</td>\n",
       "      <td>301141.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>171713.0</td>\n",
       "      <td>3851.88</td>\n",
       "      <td>13268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11306699.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>2982.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>175069.0</td>\n",
       "      <td>235062.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>59993.0</td>\n",
       "      <td>3818.24</td>\n",
       "      <td>5423.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11047729.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1757.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>162926.0</td>\n",
       "      <td>270543.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>107617.0</td>\n",
       "      <td>3450.67</td>\n",
       "      <td>26198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>17174848.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>180532.0</td>\n",
       "      <td>300886.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>120354.0</td>\n",
       "      <td>3394.26</td>\n",
       "      <td>25187.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24750</td>\n",
       "      <td>11633053.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1382.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>214623.0</td>\n",
       "      <td>786817.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>572194.0</td>\n",
       "      <td>9550.24</td>\n",
       "      <td>8421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24751</td>\n",
       "      <td>10751511.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1127.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>96981.0</td>\n",
       "      <td>282464.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>185483.0</td>\n",
       "      <td>4086.50</td>\n",
       "      <td>29610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24752</td>\n",
       "      <td>10719915.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>2179.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>102310.0</td>\n",
       "      <td>419389.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>317079.0</td>\n",
       "      <td>5259.98</td>\n",
       "      <td>17317.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24753</td>\n",
       "      <td>11639212.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>3807.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1923.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>693832.0</td>\n",
       "      <td>3437492.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>2743660.0</td>\n",
       "      <td>40656.13</td>\n",
       "      <td>6587.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24754</td>\n",
       "      <td>17273307.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1308.0</td>\n",
       "      <td>1308.0</td>\n",
       "      <td>1308.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42794.0</td>\n",
       "      <td>61103.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>18309.0</td>\n",
       "      <td>2296.28</td>\n",
       "      <td>19286.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24755 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            lotid  aircond  numbath  numbedroom  qualitybuild  \\\n",
       "0      17214744.0      1.0      2.0         3.0           6.0   \n",
       "1      12018724.0      1.0      2.0         1.0           4.0   \n",
       "2      11306699.0      1.0      3.0         4.0           4.0   \n",
       "3      11047729.0      1.0      2.0         3.0           7.0   \n",
       "4      17174848.0      1.0      3.0         2.0           6.0   \n",
       "...           ...      ...      ...         ...           ...   \n",
       "24750  11633053.0      1.0      3.0         2.0           7.0   \n",
       "24751  10751511.0      1.0      2.0         2.0           7.0   \n",
       "24752  10719915.0      1.0      2.0         4.0           6.0   \n",
       "24753  11639212.0      1.0      5.0         5.0          10.0   \n",
       "24754  17273307.0      1.0      2.0         3.0           6.0   \n",
       "\n",
       "       finishedarea1st  finishedarea  finishedareaEntry  countycode  \\\n",
       "0               1255.0        1255.0             1255.0      6111.0   \n",
       "1               1300.0        1033.0             1301.0      6037.0   \n",
       "2               1300.0        2982.0             1301.0      6037.0   \n",
       "3               1300.0        1757.0             1301.0      6037.0   \n",
       "4               1047.0        1951.0             1047.0      6111.0   \n",
       "...                ...           ...                ...         ...   \n",
       "24750           1300.0        1382.0             1301.0      6037.0   \n",
       "24751           1300.0        1127.0             1301.0      6037.0   \n",
       "24752           1300.0        2179.0             1301.0      6037.0   \n",
       "24753           1300.0        3807.0             1301.0      6037.0   \n",
       "24754           1308.0        1308.0             1308.0      6111.0   \n",
       "\n",
       "       numfireplace  ...  num34bath  unitnum    year  numstories  buildvalue  \\\n",
       "0               1.0  ...        1.0      1.0  1979.0         1.0    103993.0   \n",
       "1               0.0  ...        1.0      1.0  1973.0         2.0    129428.0   \n",
       "2               0.0  ...        1.0      1.0  2004.0         2.0    175069.0   \n",
       "3               0.0  ...        1.0      1.0  1952.0         2.0    162926.0   \n",
       "4               0.0  ...        1.0      1.0  1988.0         2.0    180532.0   \n",
       "...             ...  ...        ...      ...     ...         ...         ...   \n",
       "24750           0.0  ...        1.0      1.0  1982.0         2.0    214623.0   \n",
       "24751           0.0  ...        1.0      1.0  1973.0         2.0     96981.0   \n",
       "24752           0.0  ...        1.0      1.0  1962.0         2.0    102310.0   \n",
       "24753           0.0  ...        1.0      1.0  1923.0         2.0    693832.0   \n",
       "24754           1.0  ...        1.0      1.0  1964.0         1.0     42794.0   \n",
       "\n",
       "       parcelvalue  taxyear  landvalue  totaltaxvalue  mypointer  \n",
       "0         142212.0   2015.0    38219.0        1715.08    25711.0  \n",
       "1         301141.0   2015.0   171713.0        3851.88    13268.0  \n",
       "2         235062.0   2015.0    59993.0        3818.24     5423.0  \n",
       "3         270543.0   2015.0   107617.0        3450.67    26198.0  \n",
       "4         300886.0   2015.0   120354.0        3394.26    25187.0  \n",
       "...            ...      ...        ...            ...        ...  \n",
       "24750     786817.0   2016.0   572194.0        9550.24     8421.0  \n",
       "24751     282464.0   2015.0   185483.0        4086.50    29610.0  \n",
       "24752     419389.0   2016.0   317079.0        5259.98    17317.0  \n",
       "24753    3437492.0   2015.0  2743660.0       40656.13     6587.0  \n",
       "24754      61103.0   2015.0    18309.0        2296.28    19286.0  \n",
       "\n",
       "[24755 rows x 33 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets save a list with the names of the columns\n",
    "variable_names = list(df_imp)\n",
    "#Save the list into a txt file to use later\n",
    "with open(\"train_database_names.txt\", \"w\") as file:\n",
    "    for col in variable_names:\n",
    "        file.write(str(col) + ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **************************************************************************************** \n",
      "                   Features with no missing values:  33                    \n",
      " ****************************************************************************************\n",
      "\n",
      " **************************************************************************************** \n",
      "        Features with  15.0 % or less missing values:  0          \n",
      " ****************************************************************************************\n",
      "\n",
      " **************************************************************************************** \n",
      "   Features with missing values between 15.0 % and  35.0 % :  0   \n",
      " ****************************************************************************************\n",
      "\n",
      " **************************************************************************************** \n",
      " WARNING:  Features with  35.0 % or more missing values:  0          \n",
      " ****************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#verify that we no longer have missing data left\n",
    "missing_report(df_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy of the final df after imputation\n",
    "df_final = df_imp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature crafting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our database has an important number of categorical values that we would like to use in our model. But, we need to carefully think how to do this. Creating dummy variables for each category is one option. However, this turns out to be problematic when we have a very large number of categories and few observations per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop variables that we were told to drop\n",
    "X = df_final.drop(['parcelvalue', 'lotid', 'totaltaxvalue', 'buildvalue', 'landvalue', 'mypointer'], axis = 1)\n",
    "#this is the classificatoin outcome\n",
    "y = df_final[\"parcelvalue\"]  \n",
    "#as said before we would like to work with the logarithm of the parcel value\n",
    "y_log = np.log(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1. : Exploration of variables and creation of dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in /opt/conda/lib/python3.7/site-packages (2.1.0)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /opt/conda/lib/python3.7/site-packages (from category_encoders) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from category_encoders) (1.17.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from category_encoders) (0.21.3)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from category_encoders) (1.3.1)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in /opt/conda/lib/python3.7/site-packages (from category_encoders) (0.10.1)\n",
      "Requirement already satisfied: patsy>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.21.1->category_encoders) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.21.1->category_encoders) (2019.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->category_encoders) (0.13.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a dataframe for the transformed data\n",
    "X_trans = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aircond</th>\n",
       "      <th>numbath</th>\n",
       "      <th>numbedroom</th>\n",
       "      <th>qualitybuild</th>\n",
       "      <th>finishedarea1st</th>\n",
       "      <th>finishedarea</th>\n",
       "      <th>finishedareaEntry</th>\n",
       "      <th>countycode</th>\n",
       "      <th>numfireplace</th>\n",
       "      <th>numfullbath</th>\n",
       "      <th>...</th>\n",
       "      <th>citycode</th>\n",
       "      <th>countycode2</th>\n",
       "      <th>neighborhoodcode</th>\n",
       "      <th>regioncode</th>\n",
       "      <th>roomnum</th>\n",
       "      <th>num34bath</th>\n",
       "      <th>unitnum</th>\n",
       "      <th>year</th>\n",
       "      <th>numstories</th>\n",
       "      <th>taxyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "      <td>24755.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>1.646940</td>\n",
       "      <td>2.603514</td>\n",
       "      <td>3.297637</td>\n",
       "      <td>5.787033</td>\n",
       "      <td>1332.349586</td>\n",
       "      <td>2094.479539</td>\n",
       "      <td>1335.745304</td>\n",
       "      <td>6058.793981</td>\n",
       "      <td>0.320743</td>\n",
       "      <td>2.503777</td>\n",
       "      <td>...</td>\n",
       "      <td>28734.358271</td>\n",
       "      <td>2560.090487</td>\n",
       "      <td>122252.239144</td>\n",
       "      <td>96636.356413</td>\n",
       "      <td>2.705554</td>\n",
       "      <td>1.002545</td>\n",
       "      <td>1.003757</td>\n",
       "      <td>1972.887013</td>\n",
       "      <td>1.821127</td>\n",
       "      <td>2015.281761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.643752</td>\n",
       "      <td>1.045068</td>\n",
       "      <td>1.072567</td>\n",
       "      <td>1.741370</td>\n",
       "      <td>352.762842</td>\n",
       "      <td>1171.576602</td>\n",
       "      <td>370.699480</td>\n",
       "      <td>30.976970</td>\n",
       "      <td>0.619474</td>\n",
       "      <td>1.044157</td>\n",
       "      <td>...</td>\n",
       "      <td>32240.606572</td>\n",
       "      <td>699.388621</td>\n",
       "      <td>154260.350379</td>\n",
       "      <td>4734.032473</td>\n",
       "      <td>3.533887</td>\n",
       "      <td>0.060578</td>\n",
       "      <td>0.095265</td>\n",
       "      <td>18.740491</td>\n",
       "      <td>0.400268</td>\n",
       "      <td>0.449867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>6037.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4406.000000</td>\n",
       "      <td>1286.000000</td>\n",
       "      <td>6952.000000</td>\n",
       "      <td>95982.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1880.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1347.000000</td>\n",
       "      <td>1301.000000</td>\n",
       "      <td>6037.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12447.000000</td>\n",
       "      <td>2061.000000</td>\n",
       "      <td>48200.000000</td>\n",
       "      <td>96336.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1961.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1821.000000</td>\n",
       "      <td>1301.000000</td>\n",
       "      <td>6037.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>21412.000000</td>\n",
       "      <td>3101.000000</td>\n",
       "      <td>51906.000000</td>\n",
       "      <td>96385.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1974.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>2512.000000</td>\n",
       "      <td>1301.000000</td>\n",
       "      <td>6059.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>40227.000000</td>\n",
       "      <td>3101.000000</td>\n",
       "      <td>117954.000000</td>\n",
       "      <td>97021.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1985.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10110.000000</td>\n",
       "      <td>31415.000000</td>\n",
       "      <td>13370.000000</td>\n",
       "      <td>6111.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>396556.000000</td>\n",
       "      <td>3101.000000</td>\n",
       "      <td>764166.000000</td>\n",
       "      <td>399675.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            aircond       numbath    numbedroom  qualitybuild  \\\n",
       "count  24755.000000  24755.000000  24755.000000  24755.000000   \n",
       "mean       1.646940      2.603514      3.297637      5.787033   \n",
       "std        2.643752      1.045068      1.072567      1.741370   \n",
       "min        1.000000      0.000000      0.000000      1.000000   \n",
       "25%        1.000000      2.000000      3.000000      4.000000   \n",
       "50%        1.000000      2.500000      3.000000      6.000000   \n",
       "75%        1.000000      3.000000      4.000000      7.000000   \n",
       "max       13.000000     19.000000     11.000000     12.000000   \n",
       "\n",
       "       finishedarea1st  finishedarea  finishedareaEntry    countycode  \\\n",
       "count     24755.000000  24755.000000       24755.000000  24755.000000   \n",
       "mean       1332.349586   2094.479539        1335.745304   6058.793981   \n",
       "std         352.762842   1171.576602         370.699480     30.976970   \n",
       "min          47.000000    188.000000          47.000000   6037.000000   \n",
       "25%        1300.000000   1347.000000        1301.000000   6037.000000   \n",
       "50%        1300.000000   1821.000000        1301.000000   6037.000000   \n",
       "75%        1300.000000   2512.000000        1301.000000   6059.000000   \n",
       "max       10110.000000  31415.000000       13370.000000   6111.000000   \n",
       "\n",
       "       numfireplace   numfullbath  ...       citycode   countycode2  \\\n",
       "count  24755.000000  24755.000000  ...   24755.000000  24755.000000   \n",
       "mean       0.320743      2.503777  ...   28734.358271   2560.090487   \n",
       "std        0.619474      1.044157  ...   32240.606572    699.388621   \n",
       "min        0.000000      1.000000  ...    4406.000000   1286.000000   \n",
       "25%        0.000000      2.000000  ...   12447.000000   2061.000000   \n",
       "50%        0.000000      2.000000  ...   21412.000000   3101.000000   \n",
       "75%        1.000000      3.000000  ...   40227.000000   3101.000000   \n",
       "max        9.000000     19.000000  ...  396556.000000   3101.000000   \n",
       "\n",
       "       neighborhoodcode     regioncode       roomnum     num34bath  \\\n",
       "count      24755.000000   24755.000000  24755.000000  24755.000000   \n",
       "mean      122252.239144   96636.356413      2.705554      1.002545   \n",
       "std       154260.350379    4734.032473      3.533887      0.060578   \n",
       "min         6952.000000   95982.000000      0.000000      1.000000   \n",
       "25%        48200.000000   96336.000000      0.000000      1.000000   \n",
       "50%        51906.000000   96385.000000      0.000000      1.000000   \n",
       "75%       117954.000000   97021.000000      6.000000      1.000000   \n",
       "max       764166.000000  399675.000000     18.000000      5.000000   \n",
       "\n",
       "            unitnum          year    numstories       taxyear  \n",
       "count  24755.000000  24755.000000  24755.000000  24755.000000  \n",
       "mean       1.003757   1972.887013      1.821127   2015.281761  \n",
       "std        0.095265     18.740491      0.400268      0.449867  \n",
       "min        1.000000   1880.000000      1.000000   2015.000000  \n",
       "25%        1.000000   1961.000000      2.000000   2015.000000  \n",
       "50%        1.000000   1974.000000      2.000000   2015.000000  \n",
       "75%        1.000000   1985.500000      2.000000   2016.000000  \n",
       "max        9.000000   2016.000000      3.000000   2016.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trans.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list of features we can see that there are a few clear candidates for using a dummies (one-hot encoding) approach:\n",
    "\n",
    "1. Heating type\n",
    "2. Air conditioning type\n",
    "3. Tax year\n",
    "4. County\n",
    "\n",
    "Some other features could be candidates for one-hot encoding but due to the very large number of categories they could introduce we are pursuing a different approach. We would use tools from the *category_encoders* library to deal with these cases.\n",
    "\n",
    "1. City\n",
    "2. Year of construction\n",
    "\n",
    "Lets explore these features and create the new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 One-Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(cols=['aircond', 'heatingtype', 'taxyear', 'countycode'],\n",
       "              drop_invariant=False, handle_missing='value',\n",
       "              handle_unknown='value', return_df=True, use_cat_names=True,\n",
       "              verbose=1)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = ce.one_hot.OneHotEncoder(cols = ['aircond', 'heatingtype', 'taxyear', 'countycode'], \n",
    "                                   verbose = 1, use_cat_names = True )\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans = one_hot.fit_transform(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['aircond_1.0', 'aircond_13.0', 'aircond_11.0', 'aircond_5.0',\n",
       "       'aircond_3.0', 'aircond_12.0', 'numbath', 'numbedroom', 'qualitybuild',\n",
       "       'finishedarea1st', 'finishedarea', 'finishedareaEntry',\n",
       "       'countycode_6111.0', 'countycode_6037.0', 'countycode_6059.0',\n",
       "       'numfireplace', 'numfullbath', 'garagenum', 'garagearea',\n",
       "       'heatingtype_2.0', 'heatingtype_7.0', 'heatingtype_6.0',\n",
       "       'heatingtype_18.0', 'heatingtype_20.0', 'heatingtype_24.0',\n",
       "       'heatingtype_11.0', 'heatingtype_13.0', 'heatingtype_1.0',\n",
       "       'heatingtype_14.0', 'heatingtype_12.0', 'latitude', 'longitude',\n",
       "       'lotarea', 'poolnum', 'citycode', 'countycode2', 'neighborhoodcode',\n",
       "       'regioncode', 'roomnum', 'num34bath', 'unitnum', 'year', 'numstories',\n",
       "       'taxyear_2015.0', 'taxyear_2016.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trans.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are two different county codes, lets drop one\n",
    "X_trans = X_trans.drop(columns = 'countycode2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Categorical encoding\n",
    "\n",
    "Some of our categorical variables have a very large number of categories and, thus, transforming them into dummy variables could potentially be harmful. We will use some strategies from categorical encoding to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "#Year of construction\n",
    "print(len(X_trans.year.value_counts()))\n",
    "#City\n",
    "print(len(X_trans.citycode.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_hash_city = ce.HashingEncoder(cols = ['citycode'])\n",
    "ce_hash_city\n",
    "X_trans = ce_hash_city.fit_transform(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['col_0', 'col_1', 'col_2', 'col_3', 'col_4', 'col_5', 'col_6', 'col_7',\n",
       "       'aircond_1.0', 'aircond_13.0', 'aircond_11.0', 'aircond_5.0',\n",
       "       'aircond_3.0', 'aircond_12.0', 'numbath', 'numbedroom', 'qualitybuild',\n",
       "       'finishedarea1st', 'finishedarea', 'finishedareaEntry',\n",
       "       'countycode_6111.0', 'countycode_6037.0', 'countycode_6059.0',\n",
       "       'numfireplace', 'numfullbath', 'garagenum', 'garagearea',\n",
       "       'heatingtype_2.0', 'heatingtype_7.0', 'heatingtype_6.0',\n",
       "       'heatingtype_18.0', 'heatingtype_20.0', 'heatingtype_24.0',\n",
       "       'heatingtype_11.0', 'heatingtype_13.0', 'heatingtype_1.0',\n",
       "       'heatingtype_14.0', 'heatingtype_12.0', 'latitude', 'longitude',\n",
       "       'lotarea', 'poolnum', 'neighborhoodcode', 'regioncode', 'roomnum',\n",
       "       'num34bath', 'unitnum', 'year', 'numstories', 'taxyear_2015.0',\n",
       "       'taxyear_2016.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trans.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_added = ce_hash_city.get_feature_names()[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns added by hasher\n",
    "X_trans = X_trans.rename(columns = {f:'cities_{}'.format(f) for f in features_added})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_hash_year = ce.HashingEncoder(cols = ['year'])\n",
    "ce_hash_year\n",
    "X_trans = ce_hash_year.fit_transform(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_added = ce_hash_year.get_feature_names()[0:8]\n",
    "#rename columns added by hasher\n",
    "X_trans = X_trans.rename(columns = {f:'years_{}'.format(f) for f in features_added})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['years_col_0', 'years_col_1', 'years_col_2', 'years_col_3',\n",
       "       'years_col_4', 'years_col_5', 'years_col_6', 'years_col_7',\n",
       "       'cities_col_0', 'cities_col_1', 'cities_col_2', 'cities_col_3',\n",
       "       'cities_col_4', 'cities_col_5', 'cities_col_6', 'cities_col_7',\n",
       "       'aircond_1.0', 'aircond_13.0', 'aircond_11.0', 'aircond_5.0',\n",
       "       'aircond_3.0', 'aircond_12.0', 'numbath', 'numbedroom', 'qualitybuild',\n",
       "       'finishedarea1st', 'finishedarea', 'finishedareaEntry',\n",
       "       'countycode_6111.0', 'countycode_6037.0', 'countycode_6059.0',\n",
       "       'numfireplace', 'numfullbath', 'garagenum', 'garagearea',\n",
       "       'heatingtype_2.0', 'heatingtype_7.0', 'heatingtype_6.0',\n",
       "       'heatingtype_18.0', 'heatingtype_20.0', 'heatingtype_24.0',\n",
       "       'heatingtype_11.0', 'heatingtype_13.0', 'heatingtype_1.0',\n",
       "       'heatingtype_14.0', 'heatingtype_12.0', 'latitude', 'longitude',\n",
       "       'lotarea', 'poolnum', 'neighborhoodcode', 'regioncode', 'roomnum',\n",
       "       'num34bath', 'unitnum', 'numstories', 'taxyear_2015.0',\n",
       "       'taxyear_2016.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trans.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Ordinal features\n",
    "\n",
    "A number of features represent characteristics of a house that have meaning and increasing importance (two bathrooms are better than one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21154\n",
      "20549\n"
     ]
    }
   ],
   "source": [
    "# Latitude\n",
    "print(len(X.latitude.value_counts()))\n",
    "# Longitude\n",
    "print(len(X.longitude.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two features have such a large number of different categories that they might not be adding a lot of value to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans = X_trans.drop(['latitude', 'longitude'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453\n",
      "371\n"
     ]
    }
   ],
   "source": [
    "#Neighborhoods\n",
    "print(len(X.neighborhoodcode.value_counts()))\n",
    "#Region\n",
    "print(len(X.regioncode.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, it seems that with the **city** and the **county** we might be getting enough geographical information and, thus, there might not be a need to increase dramatically the features by adding the dummies for neighborhood or region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans = X_trans.drop(['neighborhoodcode', 'regioncode'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['years_col_0', 'years_col_1', 'years_col_2', 'years_col_3',\n",
       "       'years_col_4', 'years_col_5', 'years_col_6', 'years_col_7',\n",
       "       'cities_col_0', 'cities_col_1', 'cities_col_2', 'cities_col_3',\n",
       "       'cities_col_4', 'cities_col_5', 'cities_col_6', 'cities_col_7',\n",
       "       'aircond_1.0', 'aircond_13.0', 'aircond_11.0', 'aircond_5.0',\n",
       "       'aircond_3.0', 'aircond_12.0', 'numbath', 'numbedroom', 'qualitybuild',\n",
       "       'finishedarea1st', 'finishedarea', 'finishedareaEntry',\n",
       "       'countycode_6111.0', 'countycode_6037.0', 'countycode_6059.0',\n",
       "       'numfireplace', 'numfullbath', 'garagenum', 'garagearea',\n",
       "       'heatingtype_2.0', 'heatingtype_7.0', 'heatingtype_6.0',\n",
       "       'heatingtype_18.0', 'heatingtype_20.0', 'heatingtype_24.0',\n",
       "       'heatingtype_11.0', 'heatingtype_13.0', 'heatingtype_1.0',\n",
       "       'heatingtype_14.0', 'heatingtype_12.0', 'lotarea', 'poolnum', 'roomnum',\n",
       "       'num34bath', 'unitnum', 'numstories', 'taxyear_2015.0',\n",
       "       'taxyear_2016.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trans.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.5 : Consolidating the data\n",
    "\n",
    "At this stage we have decided to create dummy variables (one-hot encoding) for:\n",
    "1. heating\n",
    "2. aircond\n",
    "3. tax_year\n",
    "4. county\n",
    "\n",
    "We took a different approach using the Hash Enconding for:\n",
    "1. city\n",
    "2. year\n",
    "\n",
    "We have decided to exclude from the dataset:\n",
    "1. countycode2\n",
    "2. latitude\n",
    "3. longitude\n",
    "4. neighborhoodcode\n",
    "5. regioncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24755, 54)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have managed to create a reasonable amount of features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build your model and get predictions from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform training and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_trans, y, random_state = 92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a variable with the log of our target variable\n",
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relevant modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
       "                      n_jobs=None, oob_score=False, random_state=None,\n",
       "                      verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize the model for Regression\n",
    "forest = RandomForestRegressor(criterion = 'mse')\n",
    "forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Random Grid Search CV to get parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(max_depth = np.random.randint(low = 1, high = 120, size = 10),\n",
    "                 min_samples_leaf = np.random.randint(low = 1, high = 30, size = 10),\n",
    "                 max_features = ['auto'],\n",
    "                 max_leaf_nodes = np.random.randint(low = 10, high = 180, size = 10), \n",
    "                 n_estimators = np.random.randint(low = 10, high = 80, size = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rand_gridcv = RandomizedSearchCV(forest, parameters, random_state=92, \n",
    "                                 scoring='r2', cv=5, verbose=5, n_jobs = -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  50 | elapsed:   20.8s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   21.6s finished\n"
     ]
    }
   ],
   "source": [
    "forest_opt = rand_gridcv.fit(X_trans, y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49115327406247994"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CAUTION: this accuracy is predicting on the log of the parcel value\n",
    "forest_opt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 71,\n",
       " 'min_samples_leaf': 13,\n",
       " 'max_leaf_nodes': 137,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 117}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import pickle\n",
    "filename = 'tuned_forest.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(forest_opt, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 71,\n",
       " 'min_samples_leaf': 13,\n",
       " 'max_leaf_nodes': 137,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 117}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model from disk to test that it was properly saved\n",
    "forest_opt = pickle.load(open(filename, 'rb'))\n",
    "forest_opt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/24/5fe7237b2eca13ee0cfb100bec8c23f4e69ce9df852a64b0493d49dae4e0/xgboost-0.90-py2.py3-none-manylinux1_x86_64.whl (142.8MB)\n",
      "\u001b[K     |████████████████████████████████| 142.8MB 3.0MB/s eta 0:00:01   |█▍                              | 6.2MB 1.9MB/s eta 0:01:11     |██▏                             | 9.5MB 5.6MB/s eta 0:00:24     |█████▉                          | 26.2MB 3.4MB/s eta 0:00:35     |██████▎                         | 27.9MB 4.8MB/s eta 0:00:24     |███████▋                        | 34.2MB 2.3MB/s eta 0:00:47     |████████▍                       | 37.3MB 2.9MB/s eta 0:00:37     |████████▌                       | 37.8MB 2.9MB/s eta 0:00:37     |██████████                      | 45.0MB 3.8MB/s eta 0:00:27     |███████████▎                    | 50.4MB 3.6MB/s eta 0:00:26     |████████████████▏               | 72.3MB 4.1MB/s eta 0:00:18     |████████████████████            | 89.2MB 3.0MB/s eta 0:00:18     |████████████████████▏           | 90.2MB 3.0MB/s eta 0:00:18     |███████████████████████         | 103.0MB 3.4MB/s eta 0:00:12     |█████████████████████████▋      | 114.4MB 4.4MB/s eta 0:00:07     |███████████████████████████▌    | 122.6MB 3.9MB/s eta 0:00:06     |████████████████████████████▎   | 126.3MB 3.3MB/s eta 0:00:05\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.3.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.17.2)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-0.90\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=92,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBRegressor(seed=92)\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [6,8], #max tree depth for base learners\n",
    "              'learning_rate': [0.075, 0.1, 0.125], #so called `eta` value\n",
    "              'n_estimators': [50, 100], #number of trees \n",
    "              'objective':['reg:squarederror'], \n",
    "              'gamma': [0.05,0.075,0.1,0.15],\n",
    "              'tree_method':['auto'], #see documentation\n",
    "              'min_child_weight': [5,7,9,11],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'reg_alpha': [0, 0.25, 0.5], #L1 regularization (Lasso)\n",
    "              'reg_lambda':[0.9 ,1, 1.1, 1.2], #L2 regularization (Ridge)\n",
    "              'random_state': [92]}\n",
    "\n",
    "\n",
    "xgb_gridcv = GridSearchCV(xgb_model, parameters, cv=5, scoring= 'r2', verbose=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2304 candidates, totalling 11520 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   28.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:   49.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   53.0s\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:   58.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 322 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 405 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 465 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 529 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 562 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 669 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 706 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 745 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 825 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 866 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 909 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 952 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1042 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1089 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1185 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1285 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1336 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1389 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1609 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1666 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1725 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1845 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1906 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed: 14.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2162 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2229 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2296 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2365 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2505 tasks      | elapsed: 17.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 18.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2649 tasks      | elapsed: 19.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2722 tasks      | elapsed: 19.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2872 tasks      | elapsed: 20.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2949 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3026 tasks      | elapsed: 21.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3105 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 22.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 23.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3346 tasks      | elapsed: 23.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3429 tasks      | elapsed: 24.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3512 tasks      | elapsed: 25.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3597 tasks      | elapsed: 25.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3682 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3769 tasks      | elapsed: 27.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3856 tasks      | elapsed: 27.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3945 tasks      | elapsed: 28.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 29.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4125 tasks      | elapsed: 29.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4216 tasks      | elapsed: 30.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4309 tasks      | elapsed: 30.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4402 tasks      | elapsed: 31.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4497 tasks      | elapsed: 32.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4592 tasks      | elapsed: 33.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4689 tasks      | elapsed: 33.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4786 tasks      | elapsed: 34.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 35.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed: 36.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5085 tasks      | elapsed: 36.7min\n",
      "[Parallel(n_jobs=-1)]: Done 5186 tasks      | elapsed: 37.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5289 tasks      | elapsed: 38.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5392 tasks      | elapsed: 39.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5497 tasks      | elapsed: 40.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5602 tasks      | elapsed: 40.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5709 tasks      | elapsed: 44.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 47.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5925 tasks      | elapsed: 50.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed: 53.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6145 tasks      | elapsed: 56.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6256 tasks      | elapsed: 59.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6369 tasks      | elapsed: 63.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6482 tasks      | elapsed: 68.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6597 tasks      | elapsed: 71.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6712 tasks      | elapsed: 75.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed: 79.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6946 tasks      | elapsed: 82.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7065 tasks      | elapsed: 84.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed: 88.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7305 tasks      | elapsed: 93.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7426 tasks      | elapsed: 97.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7549 tasks      | elapsed: 101.6min\n",
      "[Parallel(n_jobs=-1)]: Done 7672 tasks      | elapsed: 105.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7797 tasks      | elapsed: 109.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7922 tasks      | elapsed: 113.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8049 tasks      | elapsed: 116.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8176 tasks      | elapsed: 119.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8305 tasks      | elapsed: 124.1min\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed: 129.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8565 tasks      | elapsed: 133.9min\n",
      "[Parallel(n_jobs=-1)]: Done 8696 tasks      | elapsed: 138.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8829 tasks      | elapsed: 141.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8962 tasks      | elapsed: 144.4min\n",
      "[Parallel(n_jobs=-1)]: Done 9097 tasks      | elapsed: 149.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 9232 tasks      | elapsed: 153.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9369 tasks      | elapsed: 158.6min\n",
      "[Parallel(n_jobs=-1)]: Done 9506 tasks      | elapsed: 162.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9645 tasks      | elapsed: 166.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed: 170.2min\n",
      "[Parallel(n_jobs=-1)]: Done 9925 tasks      | elapsed: 174.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10066 tasks      | elapsed: 178.9min\n",
      "[Parallel(n_jobs=-1)]: Done 10209 tasks      | elapsed: 183.9min\n",
      "[Parallel(n_jobs=-1)]: Done 10352 tasks      | elapsed: 188.9min\n",
      "[Parallel(n_jobs=-1)]: Done 10497 tasks      | elapsed: 193.8min\n",
      "[Parallel(n_jobs=-1)]: Done 10642 tasks      | elapsed: 198.8min\n",
      "[Parallel(n_jobs=-1)]: Done 10789 tasks      | elapsed: 203.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10936 tasks      | elapsed: 206.6min\n",
      "[Parallel(n_jobs=-1)]: Done 11085 tasks      | elapsed: 210.4min\n",
      "[Parallel(n_jobs=-1)]: Done 11234 tasks      | elapsed: 216.0min\n",
      "[Parallel(n_jobs=-1)]: Done 11385 tasks      | elapsed: 222.1min\n",
      "[Parallel(n_jobs=-1)]: Done 11520 out of 11520 | elapsed: 226.9min finished\n"
     ]
    }
   ],
   "source": [
    "#fit the model\n",
    "xgb_opt = xgb_gridcv.fit(X_trans, y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5099566554996715"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CAUTION: this accuracy is predicting on the log of the parcel value\n",
    "xgb_opt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.7,\n",
       " 'gamma': 0.075,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 7,\n",
       " 'n_estimators': 100,\n",
       " 'objective': 'reg:squarederror',\n",
       " 'random_state': 92,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'subsample': 0.8,\n",
       " 'tree_method': 'auto'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "import pickle\n",
    "filename = 'tuned_xgb.sav'\n",
    "pickle.dump(xgb_opt, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.7,\n",
       " 'gamma': 0.075,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 7,\n",
       " 'n_estimators': 100,\n",
       " 'objective': 'reg:squarederror',\n",
       " 'random_state': 92,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'subsample': 0.8,\n",
       " 'tree_method': 'auto'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model from disk to test that it was properly saved\n",
    "filename = 'tuned_xgb.sav'\n",
    "xgb_opt = pickle.load(open(filename, 'rb'))\n",
    "xgb_opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_opt = xgb.XGBRegressor(colsample_bytree= 0.7, gamma =0.075, learning_rate = 0.1, max_depth =8,\n",
    "                           min_child_weight=7,n_estimators =100, objective = 'reg:squarederror',\n",
    "                           random_state=92,reg_alpha=0, reg_lambda=1, subsample=0.8,\n",
    "                           tree_method='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.7, gamma=0.075,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=8, min_child_weight=7, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:squarederror',\n",
       "             random_state=92, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=0.8, tree_method='auto',\n",
       "             verbosity=1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_opt.fit(X_trans,y_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. AdaBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
       "                  n_estimators=50, random_state=92)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize the model\n",
    "ada = AdaBoostRegressor(random_state=92)\n",
    "ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'learning_rate': [1, 1.5, 2, 2.5],\n",
    "             'loss': ['linear'],\n",
    "             'n_estimators': [50,100,110]}\n",
    "\n",
    "ada_gridcv = GridSearchCV(ada, parameters, scoring = 'r2', verbose = 10, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=50 ...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=1, loss=linear, n_estimators=50, score=0.418, total=   1.4s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=50 ...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=1, loss=linear, n_estimators=50, score=0.434, total=   2.3s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=50 ...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    3.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=1, loss=linear, n_estimators=50, score=0.437, total=   2.3s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=50 ...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    6.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=1, loss=linear, n_estimators=50, score=0.443, total=   2.2s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=50 ...................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    8.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=1, loss=linear, n_estimators=50, score=0.429, total=   2.3s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=100 ..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   10.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=1, loss=linear, n_estimators=100, score=0.418, total=   1.4s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=100 ..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   11.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=1, loss=linear, n_estimators=100, score=0.438, total=   4.3s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=100 ..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   16.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=1, loss=linear, n_estimators=100, score=0.438, total=   4.2s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=100 ..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   20.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=1, loss=linear, n_estimators=100, score=0.442, total=   4.2s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=100 ..................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   24.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=1, loss=linear, n_estimators=100, score=0.429, total=   4.0s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=110 ..................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=110, score=0.418, total=   1.4s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=110 ..................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=110, score=0.437, total=   4.5s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=110 ..................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=110, score=0.437, total=   4.6s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=110 ..................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=110, score=0.442, total=   4.8s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=110 ..................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=110, score=0.429, total=   4.2s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=50, score=0.410, total=   0.9s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=50, score=0.431, total=   2.1s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=50, score=0.433, total=   2.2s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=50, score=0.447, total=   1.2s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=50, score=0.434, total=   2.2s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=100, score=0.410, total=   0.9s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=100, score=0.431, total=   2.1s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=100, score=0.435, total=   2.2s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=100, score=0.447, total=   1.2s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=100, score=0.428, total=   4.0s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=110 ................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=110, score=0.410, total=   0.9s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=110 ................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=110, score=0.431, total=   2.1s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=110 ................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=110, score=0.435, total=   2.2s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=110 ................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=110, score=0.447, total=   1.2s\n",
      "[CV] learning_rate=1.5, loss=linear, n_estimators=110 ................\n",
      "[CV]  learning_rate=1.5, loss=linear, n_estimators=110, score=0.425, total=   4.4s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=50 ...................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=50, score=0.420, total=   2.1s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=50 ...................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=50, score=0.431, total=   2.1s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=50 ...................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=50, score=0.435, total=   2.1s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=50 ...................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=50, score=0.445, total=   2.1s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=50 ...................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=50, score=0.429, total=   1.6s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=100 ..................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=100, score=0.421, total=   2.2s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=100 ..................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=100, score=0.440, total=   3.8s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=100 ..................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=100, score=0.434, total=   3.8s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=100 ..................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=100, score=0.445, total=   3.6s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=100 ..................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=100, score=0.429, total=   1.7s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=110 ..................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=110, score=0.421, total=   2.3s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=110 ..................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=110, score=0.442, total=   4.4s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=110 ..................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=110, score=0.437, total=   4.3s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=110 ..................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=110, score=0.445, total=   3.6s\n",
      "[CV] learning_rate=2, loss=linear, n_estimators=110 ..................\n",
      "[CV]  learning_rate=2, loss=linear, n_estimators=110, score=0.429, total=   1.6s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=50, score=0.421, total=   2.0s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=50, score=0.446, total=   2.0s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=50, score=0.440, total=   2.0s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=50, score=0.445, total=   1.1s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=50, score=0.426, total=   2.0s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=100, score=0.418, total=   3.8s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=100, score=0.450, total=   3.8s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=100, score=0.440, total=   3.7s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=100, score=0.445, total=   1.2s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=100, score=0.424, total=   3.8s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=110 ................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=110, score=0.419, total=   4.2s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=110 ................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=110, score=0.448, total=   4.1s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=110 ................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=110, score=0.439, total=   4.2s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=110 ................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=110, score=0.445, total=   1.2s\n",
      "[CV] learning_rate=2.5, loss=linear, n_estimators=110 ................\n",
      "[CV]  learning_rate=2.5, loss=linear, n_estimators=110, score=0.425, total=   4.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  2.7min finished\n"
     ]
    }
   ],
   "source": [
    "ada_opt = ada_gridcv.fit(X_trans, y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4354215314074092"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_opt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 2.5, 'loss': 'linear', 'n_estimators': 50}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'tuned_ada.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 2.5, 'loss': 'linear', 'n_estimators': 50}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the model\n",
    "pickle.dump(ada_opt, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 2.5, 'loss': 'linear', 'n_estimators': 50}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model from disk to test that it was properly saved\n",
    "ada_opt = pickle.load(open(filename, 'rb'))\n",
    "ada_opt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Assess expected accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Random forest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions for the training data\n",
    "y_pred_train_log = forest_opt.predict(X_train)\n",
    "y_pred_train = np.exp(y_pred_train_log)\n",
    "\n",
    "#Predictions for the test data\n",
    "y_pred_log = forest_opt.predict(X_test)\n",
    "y_pred = np.exp(y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for train data : 0.5464740550691245\n",
      "Mean Squared Error for train data: 218464713754.5819\n",
      "R2 for test data : 0.46349447684202183\n",
      "Mean Squared Error for test data: 355713162227.13806\n"
     ]
    }
   ],
   "source": [
    "#Reporting performance of this first model on the original data (without logarithms)\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "print('R2 for train data : ' + str(r2_score(y_train, y_pred_train)))\n",
    "print('Mean Squared Error for train data: ' + str(mean_squared_error(y_train, y_pred_train)))\n",
    "\n",
    "\n",
    "print('R2 for test data : ' + str(r2_score(y_test, y_pred)))\n",
    "print('Mean Squared Error for test data: ' + str(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a dictionary to store this results\n",
    "results = {'forest': {'r2':r2_score(y_test, y_pred) , 'mse': mean_squared_error(y_test, y_pred)}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forest': {'r2': 0.46349447684202183, 'mse': 355713162227.13806}}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. XGBoost accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions for the training data\n",
    "y_pred_train_log = xgb_opt.predict(X_train)\n",
    "y_pred_train = np.exp(y_pred_train_log)\n",
    "\n",
    "#Predictions for the test data\n",
    "y_pred_log = xgb_opt.predict(X_test)\n",
    "y_pred = np.exp(y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for train data : 0.6806009214403136\n",
      "Mean Squared Error for train data: 153855427789.59268\n",
      "R2 for test data : 0.6120999551700375\n",
      "Mean Squared Error for test data: 257184960114.35284\n"
     ]
    }
   ],
   "source": [
    "#Reporting performance of this first model on the original data (without logarithms)\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "print('R2 for train data : ' + str(r2_score(y_train, y_pred_train)))\n",
    "print('Mean Squared Error for train data: ' + str(mean_squared_error(y_train, y_pred_train)))\n",
    "\n",
    "\n",
    "print('R2 for test data : ' + str(r2_score(y_test, y_pred)))\n",
    "print('Mean Squared Error for test data: ' + str(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forest': {'r2': 0.46349447684202183, 'mse': 355713162227.13806},\n",
       " 'xgb': {'r2': 0.6120999551700375, 'mse': 257184960114.35284}}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add results to dictionary\n",
    "results['xgb'] = {'r2':r2_score(y_test, y_pred) , 'mse': mean_squared_error(y_test, y_pred)}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. AdaBoost accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions for the training data\n",
    "y_pred_train_log = ada_opt.predict(X_train)\n",
    "y_pred_train = np.exp(y_pred_train_log)\n",
    "\n",
    "#Predictions for the test data\n",
    "y_pred_log = ada_opt.predict(X_test)\n",
    "y_pred = np.exp(y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for train data : 0.4364682072418813\n",
      "Mean Squared Error for train data: 271454837749.74094\n",
      "R2 for test data : 0.4018673234834115\n",
      "Mean Squared Error for test data: 396573113623.7486\n"
     ]
    }
   ],
   "source": [
    "#Reporting performance of this first model on the original data (without logarithms)\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "print('R2 for train data : ' + str(r2_score(y_train, y_pred_train)))\n",
    "print('Mean Squared Error for train data: ' + str(mean_squared_error(y_train, y_pred_train)))\n",
    "\n",
    "\n",
    "print('R2 for test data : ' + str(r2_score(y_test, y_pred)))\n",
    "print('Mean Squared Error for test data: ' + str(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forest': {'r2': 0.46349447684202183, 'mse': 355713162227.13806},\n",
       " 'xgb': {'r2': 0.6120999551700375, 'mse': 257184960114.35284},\n",
       " 'ada': {'r2': 0.4018673234834115, 'mse': 396573113623.7486}}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add results to dictionary\n",
    "results['ada'] = {'r2':r2_score(y_test, y_pred) , 'mse': mean_squared_error(y_test, y_pred)}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Feature importance\n",
    "\n",
    "After getting a first evaluation of the performance of each model on the test split, now we would want to understand feature importance. This will help us in two ways. First, we could potentially discard some of the features that have very low importances and reduce the dimensionality of the problem. Second, we could use the most important features for propagation in an ensemble (we will do this in the next step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finishedarea         1.000000\n",
       "qualitybuild         0.058728\n",
       "lotarea              0.018586\n",
       "numbedroom           0.018332\n",
       "numbath              0.011747\n",
       "garagearea           0.008068\n",
       "finishedarea1st      0.007134\n",
       "finishedareaEntry    0.005076\n",
       "cities_col_6         0.003020\n",
       "years_col_7          0.002501\n",
       "cities_col_0         0.002051\n",
       "roomnum              0.001828\n",
       "numfullbath          0.001611\n",
       "countycode_6059.0    0.001529\n",
       "years_col_0          0.001399\n",
       "taxyear_2016.0       0.001236\n",
       "heatingtype_7.0      0.001215\n",
       "cities_col_5         0.001198\n",
       "numfireplace         0.001191\n",
       "taxyear_2015.0       0.001124\n",
       "years_col_4          0.000968\n",
       "years_col_2          0.000821\n",
       "cities_col_1         0.000803\n",
       "countycode_6111.0    0.000783\n",
       "years_col_5          0.000757\n",
       "numstories           0.000751\n",
       "years_col_6          0.000741\n",
       "cities_col_3         0.000712\n",
       "heatingtype_2.0      0.000695\n",
       "cities_col_7         0.000682\n",
       "garagenum            0.000608\n",
       "years_col_3          0.000517\n",
       "aircond_13.0         0.000410\n",
       "years_col_1          0.000372\n",
       "heatingtype_6.0      0.000321\n",
       "aircond_1.0          0.000291\n",
       "cities_col_2         0.000261\n",
       "countycode_6037.0    0.000218\n",
       "cities_col_4         0.000156\n",
       "aircond_5.0          0.000136\n",
       "unitnum              0.000123\n",
       "num34bath            0.000000\n",
       "aircond_11.0         0.000000\n",
       "aircond_3.0          0.000000\n",
       "aircond_12.0         0.000000\n",
       "poolnum              0.000000\n",
       "heatingtype_12.0     0.000000\n",
       "heatingtype_14.0     0.000000\n",
       "heatingtype_1.0      0.000000\n",
       "heatingtype_13.0     0.000000\n",
       "heatingtype_11.0     0.000000\n",
       "heatingtype_24.0     0.000000\n",
       "heatingtype_20.0     0.000000\n",
       "heatingtype_18.0     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = X_trans.columns\n",
    "importances = forest_opt.best_estimator_.feature_importances_\n",
    "important_features = pd.Series(data=importances/importances.max() ,index=feature_names)\n",
    "important_features.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numfullbath          1.000000\n",
       "numbath              0.510061\n",
       "finishedarea         0.496430\n",
       "qualitybuild         0.298949\n",
       "numstories           0.089855\n",
       "countycode_6111.0    0.089102\n",
       "years_col_0          0.086056\n",
       "numbedroom           0.085570\n",
       "cities_col_0         0.084096\n",
       "garagearea           0.079651\n",
       "cities_col_6         0.076788\n",
       "taxyear_2016.0       0.075524\n",
       "countycode_6059.0    0.073932\n",
       "heatingtype_6.0      0.073826\n",
       "years_col_7          0.068045\n",
       "cities_col_7         0.063442\n",
       "heatingtype_7.0      0.061699\n",
       "years_col_6          0.061408\n",
       "finishedareaEntry    0.059457\n",
       "heatingtype_2.0      0.056797\n",
       "numfireplace         0.055303\n",
       "aircond_5.0          0.053797\n",
       "lotarea              0.053507\n",
       "cities_col_3         0.053317\n",
       "finishedarea1st      0.051637\n",
       "countycode_6037.0    0.051139\n",
       "garagenum            0.049512\n",
       "cities_col_1         0.047973\n",
       "aircond_13.0         0.047864\n",
       "years_col_4          0.047790\n",
       "cities_col_5         0.047314\n",
       "roomnum              0.046857\n",
       "years_col_2          0.046426\n",
       "taxyear_2015.0       0.045757\n",
       "unitnum              0.042932\n",
       "aircond_1.0          0.041829\n",
       "cities_col_2         0.041263\n",
       "years_col_3          0.040824\n",
       "cities_col_4         0.038267\n",
       "years_col_1          0.037029\n",
       "years_col_5          0.035947\n",
       "heatingtype_18.0     0.034729\n",
       "num34bath            0.033872\n",
       "aircond_11.0         0.023320\n",
       "heatingtype_24.0     0.023237\n",
       "heatingtype_20.0     0.017935\n",
       "heatingtype_11.0     0.000000\n",
       "heatingtype_14.0     0.000000\n",
       "heatingtype_1.0      0.000000\n",
       "aircond_12.0         0.000000\n",
       "heatingtype_12.0     0.000000\n",
       "poolnum              0.000000\n",
       "heatingtype_13.0     0.000000\n",
       "aircond_3.0          0.000000\n",
       "dtype: float32"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances2 = xgb_opt.best_estimator_.feature_importances_\n",
    "important_features2 = pd.Series(data=importances2/importances2.max() ,index=feature_names)\n",
    "important_features2.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Building an ensemble\n",
    "\n",
    "We would now build an ensemble that can combine the different type of Decision Trees models we have used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Complement the ensemble with additional models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to implement a KNN model we need to scale the data\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_trans.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_trans.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a fit a simple KNN model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn_model = KNeighborsRegressor(weights = 'distance')\n",
    "parameters = {'n_neighbors':[5,20,50]}\n",
    "\n",
    "knn_gridcv = GridSearchCV(knn_model, param_grid = parameters, \n",
    "                          scoring = 'r2', cv = 3, verbose = 10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:   19.0s remaining:   38.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:   19.3s remaining:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:   34.4s remaining:   27.5s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:   35.1s remaining:   17.6s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:   35.7s remaining:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   45.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   45.8s finished\n"
     ]
    }
   ],
   "source": [
    "knn_opt = knn_gridcv.fit(X_train_scaled, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39643337418438446"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_opt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 50}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "filename = 'tuned_knn.sav'\n",
    "pickle.dump(knn_opt, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 50}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model from disk to test that it was properly saved\n",
    "knn_opt = pickle.load(open(filename, 'rb'))\n",
    "knn_opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for test data : 0.30063310406946264\n",
      "Mean Squared Error for test data: 463693288084.14856\n"
     ]
    }
   ],
   "source": [
    "#Predictions for the test data\n",
    "y_pred_log = knn_opt.predict(X_test)\n",
    "y_pred = np.exp(y_pred_log)\n",
    "\n",
    "#Reporting performance of this first model on the original data (without logarithms)\n",
    "print('R2 for test data : ' + str(r2_score(y_test, y_pred)))\n",
    "print('Mean Squared Error for test data: ' + str(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although I tried KNN the results are not good and I will not use the model for my ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a fit a different Gradient Boosting model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb_model = GradientBoostingRegressor(random_state=92, subsample=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'learning_rate': [0.1,0.2,0.5],\n",
    "                 'max_depth': [6,8,10],\n",
    "                 'n_estimators': [80,100],\n",
    "                 'alpha': [0.2,0.5,0.8]}\n",
    "\n",
    "gb_gridcv = RandomizedSearchCV(gb_model, param_distributions = parameters, \n",
    "                          scoring = 'r2', cv = 5, verbose = 10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  7.6min finished\n"
     ]
    }
   ],
   "source": [
    "gb_opt = gb_gridcv.fit(X_trans, y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5046644285977236"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_opt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_opt.best_params_\n",
    "\n",
    "#Save the model\n",
    "filename = 'tuned_gb.sav'\n",
    "pickle.dump(gb_opt, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for test data : 0.6283352593617879\n",
      "Mean Squared Error for test data: 246420650811.8621\n"
     ]
    }
   ],
   "source": [
    "#Predictions for the test data\n",
    "y_pred_log = gb_opt.predict(X_test)\n",
    "y_pred = np.exp(y_pred_log)\n",
    "\n",
    "#Reporting performance of this first model on the original data (without logarithms)\n",
    "print('R2 for test data : ' + str(r2_score(y_test, y_pred)))\n",
    "print('Mean Squared Error for test data: ' + str(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forest': {'r2': 0.46349447684202183, 'mse': 355713162227.13806},\n",
       " 'xgb': {'r2': 0.6120999551700375, 'mse': 257184960114.35284},\n",
       " 'ada': {'r2': 0.4018673234834115, 'mse': 396573113623.7486},\n",
       " 'gb': {'r2': 0.6283352593617879, 'mse': 246420650811.8621}}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add results to dictionary\n",
    "results['gb'] = {'r2':r2_score(y_test, y_pred) , 'mse': mean_squared_error(y_test, y_pred)}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Building a first ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlens in /opt/conda/lib/python3.7/site-packages (0.2.3)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.7/site-packages (from mlens) (1.17.2)\n",
      "Requirement already satisfied: scipy>=0.17 in /opt/conda/lib/python3.7/site-packages (from mlens) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install mlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so lets start by creating a dictionary with our models!\n",
    "models = {'forest': forest_opt.best_estimator_, \n",
    "          'xgb': xgb_opt.best_estimator_, \n",
    "          'ada': ada_opt.best_estimator_,\n",
    "         'gb': gb_opt.best_estimator_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=117,\n",
       "                      max_features='auto', max_leaf_nodes=137,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=13, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=71,\n",
       "                      n_jobs=None, oob_score=False, random_state=None,\n",
       "                      verbose=0, warm_start=False), XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.7, gamma=0.075,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=8, min_child_weight=7, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:squarederror',\n",
       "             random_state=92, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=0.8, tree_method='auto',\n",
       "             verbosity=1), AdaBoostRegressor(base_estimator=None, learning_rate=2.5, loss='linear',\n",
       "                  n_estimators=50, random_state=92), GradientBoostingRegressor(alpha=0.8, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=6,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=92, subsample=0.8, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "meta_learner = ExtraTreesRegressor(n_estimators=60,bootstrap=True, max_leaf_nodes = 155, \n",
    "                                   max_features=0.7,random_state=92, min_samples_leaf = 20,\n",
    "                                   max_depth = 90 , criterion = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearner(array_check=None, backend=None, folds=5,\n",
       "       layers=[Layer(backend='threading', dtype=<class 'numpy.float32'>, n_jobs=-1,\n",
       "   name='layer-1', propagate_features=None, raise_on_exception=True,\n",
       "   random_state=4218, shuffle=False,\n",
       "   stack=[Group(backend='threading', dtype=<class 'numpy.float32'>,\n",
       "   indexer=FoldIndex(X=None, folds=5, raise_on_ex...cf19f28>)],\n",
       "   n_jobs=-1, name='group-7', raise_on_exception=True, transformers=[])],\n",
       "   verbose=9)],\n",
       "       model_selection=False, n_jobs=None, raise_on_exception=True,\n",
       "       random_state=92, sample_size=20,\n",
       "       scorer=<function r2_score at 0x7efe2cf19f28>, shuffle=False,\n",
       "       verbose=10)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "# Initialize the ensemble with 5 folds (stacking meta-learner)\n",
    "super_learner = SuperLearner(\n",
    "    folds=5,\n",
    "    random_state=92,\n",
    "    verbose=10,\n",
    "    backend=\"multiprocessing\",\n",
    "    scorer = r2_score,\n",
    "    n_jobs= -1\n",
    ")\n",
    "\n",
    "# Add the base learners (layer 1)\n",
    "super_learner.add(list(models.values()), proba = False)\n",
    "\n",
    "# Add the meta learner (layer 2)\n",
    "super_learner.add_meta(meta_learner, proba = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 2 layers\n",
      "[INFO] n_jobs = -1\n",
      "[INFO] backend = 'multiprocessing'\n",
      "[INFO] start_method = 'fork'\n",
      "[INFO] cache = '/tmp'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer-1            \n",
      "Learners ...                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adaboostregressor.0.0          done | 00:00:06\n",
      "adaboostregressor.0.3          done | 00:00:09\n",
      "adaboostregressor.0.2          done | 00:00:09\n",
      "adaboostregressor.0.1          done | 00:00:10\n",
      "adaboostregressor.0.4          done | 00:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   11.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adaboostregressor.0.5          done | 00:00:07\n",
      "gradientboostingregressor.0.1  done | 00:00:20\n",
      "gradientboostingregressor.0.2  done | 00:00:19\n",
      "gradientboostingregressor.0.0  done | 00:00:26\n",
      "gradientboostingregressor.0.3  done | 00:00:18\n",
      "gradientboostingregressor.0.4  done | 00:00:18\n",
      "gradientboostingregressor.0.5  done | 00:00:17\n",
      "randomforestregressor.0.1      done | 00:00:15\n",
      "randomforestregressor.0.0      done | 00:00:20\n",
      "randomforestregressor.0.2      done | 00:00:17\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "randomforestregressor.0.3      done | 00:00:17\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "randomforestregressor.0.4      done | 00:00:16\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "randomforestregressor.0.5      done | 00:00:14\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "xgbregressor.0.1               done | 00:00:09\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "xgbregressor.0.2               done | 00:00:09\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "xgbregressor.0.0               done | 00:00:12\n",
      "xgbregressor.0.3               done | 00:00:10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  1.4min remaining:    7.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xgbregressor.0.4               done | 00:00:09\n",
      "xgbregressor.0.5               done | 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  1.4min finished\n",
      "done | 00:01:26\n",
      "layer-1                        done | 00:01:27\n",
      "Processing layer-2            \n",
      "Learners ...                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extratreesregressor.0.0        done | 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.4s finished\n",
      "done | 00:00:00\n",
      "layer-2                        done | 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fit complete                        | 00:01:28\n"
     ]
    }
   ],
   "source": [
    "# Train the ensemble\n",
    "ensemble = super_learner.fit(X_trans, y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting 2 layers\n",
      "[INFO] n_jobs = -1\n",
      "[INFO] backend = 'multiprocessing'\n",
      "[INFO] start_method = 'fork'\n",
      "[INFO] cache = '/tmp'\n",
      "\n",
      "adaboostregressor.0.0          done | 00:00:00\n",
      "gradientboostingregressor.0.0  done | 00:00:00\n",
      "randomforestregressor.0.0      done | 00:00:00\n",
      "xgbregressor.0.0               done | 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer-1            \n",
      "Learners ...                  \n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extratreesregressor.0.0        done | 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "done | 00:00:00\n",
      "layer-1                        done | 00:00:00\n",
      "Processing layer-2            \n",
      "Learners ...                  \n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "done | 00:00:00\n",
      "layer-2                        done | 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict complete                    | 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#Predictions for the test data\n",
    "y_pred_log = ensemble.predict(X_test)\n",
    "y_pred = np.exp(y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for test data : 0.4456679120286967\n",
      "Mean Squared Error for test data: 367532506982.5651\n"
     ]
    }
   ],
   "source": [
    "#Reporting performance of the ensemble on the original data (without logarithms)\n",
    "print('R2 for test data : ' + str(r2_score(y_test, y_pred)))\n",
    "print('Mean Squared Error for test data: ' + str(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forest': {'r2': 0.46349447684202183, 'mse': 355713162227.13806},\n",
       " 'xgb': {'r2': 0.6120999551700375, 'mse': 257184960114.35284},\n",
       " 'ada': {'r2': 0.4018673234834115, 'mse': 396573113623.7486},\n",
       " 'gb': {'r2': 0.6283352593617879, 'mse': 246420650811.8621},\n",
       " 'ensemble': {'r2': 0.4456679120286967, 'mse': 367532506982.5651}}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add results to dictionary\n",
    "results['ensemble'] = {'r2':r2_score(y_test, y_pred) , 'mse': mean_squared_error(y_test, y_pred)}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Ensemble with feature propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['numbath', 'numbedroom', 'qualitybuild', 'finishedarea', 'lotarea'], dtype='object')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets define the features we will propagate. We will use the top feautres from XGBoost\n",
    "to_propagate=['finishedarea', 'numbath','qualitybuild', 'lotarea', 'numbedroom']\n",
    "pointer= [i for i,x in enumerate(X_trans.columns) if x in to_propagate]\n",
    "X_trans.columns[pointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearner(array_check=None, backend=None, folds=5,\n",
       "       layers=[Layer(backend='threading', dtype=<class 'numpy.float32'>, n_jobs=-1,\n",
       "   name='layer-1', propagate_features=[22, 23, 24, 26, 46],\n",
       "   raise_on_exception=True, random_state=4218, shuffle=False,\n",
       "   stack=[Group(backend='threading', dtype=<class 'numpy.float32'>,\n",
       "   indexer=FoldIndex(X=None, fold...cf19f28>)],\n",
       "   n_jobs=-1, name='group-9', raise_on_exception=True, transformers=[])],\n",
       "   verbose=9)],\n",
       "       model_selection=False, n_jobs=None, raise_on_exception=True,\n",
       "       random_state=92, sample_size=20,\n",
       "       scorer=<function r2_score at 0x7efe2cf19f28>, shuffle=False,\n",
       "       verbose=10)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the ensemble with 5 folds (stacking meta-learner)\n",
    "super_learner = SuperLearner(\n",
    "    folds=5,\n",
    "    random_state=92,\n",
    "    verbose=10,\n",
    "    backend=\"multiprocessing\",\n",
    "    scorer = r2_score,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Add the base learners (layer 1) with feature propagation\n",
    "super_learner.add(list(models.values()), proba = False, propagate_features=pointer)\n",
    "\n",
    "# Add the meta learner (layer 2)\n",
    "super_learner.add_meta(meta_learner, proba = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 2 layers\n",
      "[INFO] n_jobs = -1\n",
      "[INFO] backend = 'multiprocessing'\n",
      "[INFO] start_method = 'fork'\n",
      "[INFO] cache = '/tmp'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer-1            \n",
      "Learners ...                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adaboostregressor.0.0          done | 00:00:06\n",
      "adaboostregressor.0.2          done | 00:00:08\n",
      "adaboostregressor.0.3          done | 00:00:08\n",
      "adaboostregressor.0.1          done | 00:00:08\n",
      "adaboostregressor.0.4          done | 00:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   10.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adaboostregressor.0.5          done | 00:00:07\n",
      "gradientboostingregressor.0.0  done | 00:00:31\n",
      "gradientboostingregressor.0.1  done | 00:00:32\n",
      "gradientboostingregressor.0.2  done | 00:00:33\n",
      "gradientboostingregressor.0.3  done | 00:00:33\n",
      "randomforestregressor.0.1      done | 00:00:19\n",
      "randomforestregressor.0.0      done | 00:00:26\n",
      "gradientboostingregressor.0.4  done | 00:00:31\n",
      "gradientboostingregressor.0.5  done | 00:00:30\n",
      "randomforestregressor.0.2      done | 00:00:14\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "randomforestregressor.0.3      done | 00:00:14\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "randomforestregressor.0.4      done | 00:00:13\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "randomforestregressor.0.5      done | 00:00:13\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "xgbregressor.0.2               done | 00:00:10\n",
      "xgbregressor.0.1               done | 00:00:10\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "xgbregressor.0.0               done | 00:00:13\n",
      "xgbregressor.0.3               done | 00:00:10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  1.6min remaining:    8.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xgbregressor.0.5               done | 00:00:07\n",
      "xgbregressor.0.4               done | 00:00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  1.7min finished\n",
      "done | 00:01:42\n",
      "layer-1                        done | 00:01:42\n",
      "Processing layer-2            \n",
      "Learners ...                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extratreesregressor.0.0        done | 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.7s finished\n",
      "done | 00:00:00\n",
      "layer-2                        done | 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fit complete                        | 00:01:44\n"
     ]
    }
   ],
   "source": [
    "# Train the ensemble\n",
    "ensemble_prop = super_learner.fit(np.array(X_trans), y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting 2 layers\n",
      "[INFO] n_jobs = -1\n",
      "[INFO] backend = 'multiprocessing'\n",
      "[INFO] start_method = 'fork'\n",
      "[INFO] cache = '/tmp'\n",
      "\n",
      "adaboostregressor.0.0          done | 00:00:00\n",
      "gradientboostingregressor.0.0  done | 00:00:00\n",
      "randomforestregressor.0.0      done | 00:00:00\n",
      "xgbregressor.0.0               done | 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer-1            \n",
      "Learners ...                  \n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "done | 00:00:00\n",
      "layer-1                        done | 00:00:00\n",
      "Processing layer-2            \n",
      "Learners ...                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extratreesregressor.0.0        done | 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "done | 00:00:00\n",
      "layer-2                        done | 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict complete                    | 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for test data : 0.4936287583556782\n",
      "Mean Squared Error for test data: 335733571885.6065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'forest': {'r2': 0.46349447684202183, 'mse': 355713162227.13806},\n",
       " 'xgb': {'r2': 0.6120999551700375, 'mse': 257184960114.35284},\n",
       " 'ada': {'r2': 0.4018673234834115, 'mse': 396573113623.7486},\n",
       " 'gb': {'r2': 0.6283352593617879, 'mse': 246420650811.8621},\n",
       " 'ensemble': {'r2': 0.4456679120286967, 'mse': 367532506982.5651},\n",
       " 'ensemble_propagation': {'r2': 0.4936287583556782, 'mse': 335733571885.6065}}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predictions for the test data\n",
    "y_pred_log = ensemble_prop.predict(np.array(X_test))\n",
    "y_pred = np.exp(y_pred_log)\n",
    "\n",
    "#Reporting performance of the ensemble on the original data (without logarithms)\n",
    "print('R2 for test data : ' + str(r2_score(y_test, y_pred)))\n",
    "print('Mean Squared Error for test data: ' + str(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "#add results to dictionary\n",
    "results['ensemble_propagation'] = {'r2':r2_score(y_test, y_pred) , 'mse': mean_squared_error(y_test, y_pred)}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. Tune the ensemble\n",
    "\n",
    "Lets look for different options for our meta-learner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_learner = xgb.XGBRegressor(colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, \n",
    "                                max_depth=8, min_child_weight=7, n_estimators=80,\n",
    "                                objective='reg:squarederror',random_state=92,\n",
    "                                reg_alpha=0, reg_lambda=0, subsample=0.8, \n",
    "                                tree_method='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_learner = GradientBoostingRegressor(n_estimators=80, max_depth=6, learning_rate=0.1, \n",
    "                                         random_state=92, subsample=0.8, alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearner(array_check=None, backend=None, folds=5,\n",
       "       layers=[Layer(backend='threading', dtype=<class 'numpy.float32'>, n_jobs=-1,\n",
       "   name='layer-1', propagate_features=[22, 23, 24, 26, 46],\n",
       "   raise_on_exception=True, random_state=4218, shuffle=False,\n",
       "   stack=[Group(backend='threading', dtype=<class 'numpy.float32'>,\n",
       "   indexer=FoldIndex(X=None, fold...f19f28>)],\n",
       "   n_jobs=-1, name='group-11', raise_on_exception=True, transformers=[])],\n",
       "   verbose=9)],\n",
       "       model_selection=False, n_jobs=None, raise_on_exception=True,\n",
       "       random_state=92, sample_size=20,\n",
       "       scorer=<function r2_score at 0x7efe2cf19f28>, shuffle=False,\n",
       "       verbose=10)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the ensemble with 5 folds (stacking meta-learner)\n",
    "super_learner = SuperLearner(\n",
    "    folds=5,\n",
    "    random_state=92,\n",
    "    verbose=10,\n",
    "    backend=\"multiprocessing\",\n",
    "    scorer = r2_score,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Add the base learners (layer 1) with feature propagation\n",
    "super_learner.add(list(models.values()), proba = False, propagate_features=pointer)\n",
    "\n",
    "# Add the meta learner (layer 2)\n",
    "super_learner.add_meta(meta_learner, proba = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 2 layers\n",
      "[INFO] n_jobs = -1\n",
      "[INFO] backend = 'multiprocessing'\n",
      "[INFO] start_method = 'fork'\n",
      "[INFO] cache = '/tmp'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer-1            \n",
      "Learners ...                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adaboostregressor.0.0          done | 00:00:06\n",
      "adaboostregressor.0.2          done | 00:00:09\n",
      "adaboostregressor.0.3          done | 00:00:09\n",
      "adaboostregressor.0.1          done | 00:00:10\n",
      "adaboostregressor.0.4          done | 00:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   12.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adaboostregressor.0.5          done | 00:00:07\n",
      "gradientboostingregressor.0.0  done | 00:00:30\n",
      "gradientboostingregressor.0.1  done | 00:00:33\n",
      "gradientboostingregressor.0.2  done | 00:00:33\n",
      "gradientboostingregressor.0.3  done | 00:00:32\n",
      "randomforestregressor.0.1      done | 00:00:18\n",
      "randomforestregressor.0.0      done | 00:00:23\n",
      "gradientboostingregressor.0.4  done | 00:00:31\n",
      "gradientboostingregressor.0.5  done | 00:00:29\n",
      "randomforestregressor.0.2      done | 00:00:20\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "randomforestregressor.0.3      done | 00:00:20\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "randomforestregressor.0.4      done | 00:00:19\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "randomforestregressor.0.5      done | 00:00:18\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "xgbregressor.0.1               done | 00:00:10\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "xgbregressor.0.2               done | 00:00:10\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "xgbregressor.0.0               done | 00:00:14\n",
      "xgbregressor.0.3               done | 00:00:11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  1.7min remaining:    9.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xgbregressor.0.4               done | 00:00:10\n",
      "xgbregressor.0.5               done | 00:00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  1.8min finished\n",
      "done | 00:01:50\n",
      "layer-1                        done | 00:01:50\n",
      "Processing layer-2            \n",
      "Learners ...                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gradientboostingregressor.0.0  done | 00:00:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.9s finished\n",
      "done | 00:00:03\n",
      "layer-2                        done | 00:00:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fit complete                        | 00:01:55\n"
     ]
    }
   ],
   "source": [
    "# Train the ensemble\n",
    "ensemble_prop2 = super_learner.fit(np.array(X_trans), y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting 2 layers\n",
      "[INFO] n_jobs = -1\n",
      "[INFO] backend = 'multiprocessing'\n",
      "[INFO] start_method = 'fork'\n",
      "[INFO] cache = '/tmp'\n",
      "\n",
      "adaboostregressor.0.0          done | 00:00:00\n",
      "gradientboostingregressor.0.0  done | 00:00:00\n",
      "randomforestregressor.0.0      done | 00:00:00\n",
      "xgbregressor.0.0               done | 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer-1            \n",
      "Learners ...                  \n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "done | 00:00:00\n",
      "layer-1                        done | 00:00:00\n",
      "Processing layer-2            \n",
      "Learners ...                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gradientboostingregressor.0.0  done | 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "done | 00:00:00\n",
      "layer-2                        done | 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict complete                    | 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for test data : 0.6123840199674455\n",
      "Mean Squared Error for test data: 256996619858.75085\n"
     ]
    }
   ],
   "source": [
    "#Predictions for the test data\n",
    "y_pred_log = ensemble_prop2.predict(np.array(X_test))\n",
    "y_pred = np.exp(y_pred_log)\n",
    "\n",
    "#Reporting performance of the ensemble on the original data (without logarithms)\n",
    "print('R2 for test data : ' + str(r2_score(y_test, y_pred)))\n",
    "print('Mean Squared Error for test data: ' + str(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forest': {'r2': 0.46349447684202183, 'mse': 355713162227.13806},\n",
       " 'xgb': {'r2': 0.6120999551700375, 'mse': 257184960114.35284},\n",
       " 'ada': {'r2': 0.4018673234834115, 'mse': 396573113623.7486},\n",
       " 'gb': {'r2': 0.6283352593617879, 'mse': 246420650811.8621},\n",
       " 'ensemble': {'r2': 0.4456679120286967, 'mse': 367532506982.5651},\n",
       " 'ensemble_propagation': {'r2': 0.4936287583556782, 'mse': 335733571885.6065},\n",
       " 'ensemble_propagation_gb': {'r2': 0.6123840199674455,\n",
       "  'mse': 256996619858.75085}}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add results to dictionary\n",
    "results['ensemble_propagation_gb'] = {'r2':r2_score(y_test, y_pred) , 'mse': mean_squared_error(y_test, y_pred)}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how we doo with XGB as the meta learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_learner = xgb.XGBRegressor(colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, \n",
    "                                max_depth=8, min_child_weight=7, n_estimators=80,\n",
    "                                objective='reg:squarederror',random_state=92,\n",
    "                                reg_alpha=0, reg_lambda=0, subsample=0.8, \n",
    "                                tree_method='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearner(array_check=None, backend=None, folds=5,\n",
       "       layers=[Layer(backend='threading', dtype=<class 'numpy.float32'>, n_jobs=-1,\n",
       "   name='layer-1', propagate_features=[22, 23, 24, 26, 46],\n",
       "   raise_on_exception=True, random_state=4218, shuffle=False,\n",
       "   stack=[Group(backend='threading', dtype=<class 'numpy.float32'>,\n",
       "   indexer=FoldIndex(X=None, fold...f19f28>)],\n",
       "   n_jobs=-1, name='group-13', raise_on_exception=True, transformers=[])],\n",
       "   verbose=9)],\n",
       "       model_selection=False, n_jobs=None, raise_on_exception=True,\n",
       "       random_state=92, sample_size=20,\n",
       "       scorer=<function r2_score at 0x7efe2cf19f28>, shuffle=False,\n",
       "       verbose=10)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the ensemble with 5 folds (stacking meta-learner)\n",
    "super_learner = SuperLearner(\n",
    "    folds=5,\n",
    "    random_state=92,\n",
    "    verbose=10,\n",
    "    backend=\"multiprocessing\",\n",
    "    scorer = r2_score,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Add the base learners (layer 1) with feature propagation\n",
    "super_learner.add(list(models.values()), proba = False, propagate_features=pointer)\n",
    "\n",
    "# Add the meta learner (layer 2)\n",
    "super_learner.add_meta(meta_learner, proba = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 2 layers\n",
      "[INFO] n_jobs = -1\n",
      "[INFO] backend = 'multiprocessing'\n",
      "[INFO] start_method = 'fork'\n",
      "[INFO] cache = '/tmp'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer-1            \n",
      "Learners ...                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adaboostregressor.0.0          done | 00:00:05\n",
      "adaboostregressor.0.2          done | 00:00:08\n",
      "adaboostregressor.0.1          done | 00:00:08\n",
      "adaboostregressor.0.3          done | 00:00:09\n",
      "adaboostregressor.0.4          done | 00:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   10.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adaboostregressor.0.5          done | 00:00:07\n",
      "gradientboostingregressor.0.0  done | 00:00:34\n",
      "gradientboostingregressor.0.1  done | 00:00:35\n",
      "gradientboostingregressor.0.2  done | 00:00:36\n",
      "gradientboostingregressor.0.3  done | 00:00:34\n",
      "randomforestregressor.0.1      done | 00:00:20\n",
      "randomforestregressor.0.0      done | 00:00:27\n",
      "gradientboostingregressor.0.4  done | 00:00:31\n",
      "gradientboostingregressor.0.5  done | 00:00:31\n",
      "randomforestregressor.0.2      done | 00:00:15\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "randomforestregressor.0.4      done | 00:00:13\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "randomforestregressor.0.3      done | 00:00:14\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "randomforestregressor.0.5      done | 00:00:13\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "xgbregressor.0.1               done | 00:00:10\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "xgbregressor.0.2               done | 00:00:10\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "xgbregressor.0.3               done | 00:00:10\n",
      "xgbregressor.0.0               done | 00:00:13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  1.7min remaining:    9.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xgbregressor.0.4               done | 00:00:07\n",
      "xgbregressor.0.5               done | 00:00:07\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/conda/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  1.8min finished\n",
      "done | 00:01:45\n",
      "layer-1                        done | 00:01:45\n",
      "Processing layer-2            \n",
      "Learners ...                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xgbregressor.0.0               done | 00:00:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.7s finished\n",
      "done | 00:00:02\n",
      "layer-2                        done | 00:00:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fit complete                        | 00:01:49\n"
     ]
    }
   ],
   "source": [
    "# Train the ensemble\n",
    "ensemble_prop3 = super_learner.fit(np.array(X_trans), y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting 2 layers\n",
      "[INFO] n_jobs = -1\n",
      "[INFO] backend = 'multiprocessing'\n",
      "[INFO] start_method = 'fork'\n",
      "[INFO] cache = '/tmp'\n",
      "\n",
      "gradientboostingregressor.0.0  done | 00:00:00\n",
      "adaboostregressor.0.0          done | 00:00:00\n",
      "randomforestregressor.0.0      done | 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer-1            \n",
      "Learners ...                  \n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:    0.2s remaining:    0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xgbregressor.0.0               done | 00:00:00\n",
      "xgbregressor.0.0               done | 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.4s finished\n",
      "done | 00:00:00\n",
      "layer-1                        done | 00:00:00\n",
      "Processing layer-2            \n",
      "Learners ...                  \n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "done | 00:00:00\n",
      "layer-2                        done | 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict complete                    | 00:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for test data : 0.6003115412629458\n",
      "Mean Squared Error for test data: 265000898268.8221\n"
     ]
    }
   ],
   "source": [
    "#Predictions for the test data\n",
    "y_pred_log = ensemble_prop3.predict(np.array(X_test))\n",
    "y_pred = np.exp(y_pred_log)\n",
    "\n",
    "#Reporting performance of the ensemble on the original data (without logarithms)\n",
    "print('R2 for test data : ' + str(r2_score(y_test, y_pred)))\n",
    "print('Mean Squared Error for test data: ' + str(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forest': {'r2': 0.46349447684202183, 'mse': 355713162227.13806},\n",
       " 'xgb': {'r2': 0.6120999551700375, 'mse': 257184960114.35284},\n",
       " 'ada': {'r2': 0.4018673234834115, 'mse': 396573113623.7486},\n",
       " 'gb': {'r2': 0.6283352593617879, 'mse': 246420650811.8621},\n",
       " 'ensemble': {'r2': 0.4456679120286967, 'mse': 367532506982.5651},\n",
       " 'ensemble_propagation': {'r2': 0.4936287583556782, 'mse': 335733571885.6065},\n",
       " 'ensemble_propagation_gb': {'r2': 0.6123840199674455,\n",
       "  'mse': 256996619858.75085},\n",
       " 'ensemble_propagation_xgb': {'r2': 0.6003115412629458,\n",
       "  'mse': 265000898268.8221}}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add results to dictionary\n",
    "results['ensemble_propagation_xgb'] = {'r2':r2_score(y_test, y_pred) , 'mse': mean_squared_error(y_test, y_pred)}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these results, my best ensemble is the one that uses feature propagation and uses GradientBosstingRegressor as the metalearner. I will save this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "filename = 'tuned_ensemble.sav'\n",
    "pickle.dump(ensemble_prop2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets save a list with the names of the columns we used to train our model\n",
    "training_features_names = list(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the list into a txt file to use later\n",
    "with open(\"training_features.txt\", \"w\") as file:\n",
    "    for col in training_features_names:\n",
    "        file.write(str(col) + ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant modules\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    \"\"\"This funcion takes as a parameter the path of a .csv file and returns\n",
    "    a Pandas DataFrame containing the data sotred in the file\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_missing(df, drop_percent=0.8, imputer=None):\n",
    "    \"\"\"This funcion takes a DataFrame as an argument an makes two major operations\n",
    "    with regards to missing data:\n",
    "        \n",
    "        1. eliminates the columns with a higer value of missing values than the\n",
    "        number specified as a parameter, and\n",
    "        2. uses a SimpleImputer to input the median on the rest of the missing values\n",
    "    \"\"\"\n",
    "    \n",
    "    missing = df.isna().sum()/df.shape[0]\n",
    "    high_missing = df.columns[missing > drop_percent]\n",
    "\n",
    "    df2 = df.drop(axis=1,labels = high_missing)\n",
    "\n",
    "    if imputer == None:\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        simp_imputer = SimpleImputer(strategy = 'median')\n",
    "        df_imp = simp_imputer.fit_transform(df2)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            df_imp = imputer.transform(df2)\n",
    "        except:\n",
    "            from sklearn.impute import SimpleImputer\n",
    "            simp_imputer = SimpleImputer(strategy = 'median')\n",
    "            df_imp = simp_imputer.fit_transform(df2)\n",
    "        \n",
    "    #reconvert the imputed data into a pandas DataFrame\n",
    "    df_imp = pd.DataFrame(df_imp, columns = df2.columns)\n",
    "\n",
    "    return(df_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_features(df):\n",
    "    \"\"\"This funcion takes as an argument a DataFrame and splits it into \n",
    "    the variable to predict (Y) and its features (X's)\n",
    "    \"\"\"\n",
    "    \n",
    "    #drop variables that we were told to drop\n",
    "    to_drop = ['parcelvalue', 'lotid', 'totaltaxvalue', 'buildvalue', 'landvalue', 'mypointer']\n",
    "    for f in to_drop:\n",
    "        try:\n",
    "            X = df.drop([f], axis = 1)\n",
    "        except:\n",
    "            print(\"WARNING: the variable \", f , \"was not found in the data provided.\")\n",
    " \n",
    "    #separate the variable to predict\n",
    "    y = df[\"parcelvalue\"]  \n",
    "    \n",
    "    return(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_crafting(X):\n",
    "    \"\"\"This funcion takes as an argument a DataFrame with the features and transforms \n",
    "    categorical variables and performs one of the following actions \n",
    "    (mimicking the process done for the training data):\n",
    "    \n",
    "        1.generates the required number of dummies\n",
    "        2.uses a different type of categorical encoding\n",
    "        3.drops the features with no correspondance on the training data\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    #1. Create dummies from the same variables as in the train dataset\n",
    "    one_hot = ce.one_hot.OneHotEncoder(cols = ['aircond', 'heatingtype', 'taxyear', 'countycode'], \n",
    "                                   verbose = 1, use_cat_names = True )\n",
    "    \n",
    "    X_trans = one_hot.fit_transform(X)\n",
    "\n",
    "    #2. Categorical encoding\n",
    "    ce_hash_city = ce.HashingEncoder(cols = ['citycode'])\n",
    "    ce_hash_city\n",
    "    X_trans = ce_hash_city.fit_transform(X_trans)\n",
    "    features_added = ce_hash_city.get_feature_names()[0:8]\n",
    "    #rename columns added by hasher\n",
    "    X_trans = X_trans.rename(columns = {f:'cities_{}'.format(f) for f in features_added})\n",
    "    \n",
    "    ce_hash_year = ce.HashingEncoder(cols = ['year'])\n",
    "    X_trans = ce_hash_year.fit_transform(X_trans)\n",
    "    features_added = ce_hash_year.get_feature_names()[0:8]\n",
    "    #rename columns added by hasher\n",
    "    X_trans = X_trans.rename(columns = {f:'years_{}'.format(f) for f in features_added})\n",
    "    \n",
    "    \n",
    "    # Drop all the columns that are not in the train dataset\n",
    "    # Read the file with the list of columns used in training\n",
    "    with open(\"training_features.txt\", \"r\") as file:\n",
    "        cols_training = file.read().split(',')\n",
    "    \n",
    "    cols_training = cols_training[:-1] \n",
    "    cols_training_filter = [i for i in cols_training if i in list(X_trans)]\n",
    "    \n",
    "    X_final = X_trans[cols_training_filter]\n",
    "    \n",
    "    #However, now there might be columns that were in the training dataset but not in the test new\n",
    "    #We are going to add them with zero values for all rows\n",
    "    \n",
    "    not_in_test = [i for i in cols_training if i not in list(X_final)]\n",
    "    \n",
    "    for c in not_in_test:\n",
    "        print(\"Column \", c, \" added to the dataset.\")\n",
    "        X_final = X_final.join(pd.DataFrame({c: [0]*X_final.shape[0]}))\n",
    "        \n",
    "    #lastly, we need to give the columns the same order as they had in the training data\n",
    "    X_final = X_final[cols_training]\n",
    "    \n",
    "    return X_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(model, X_test, y_test):\n",
    "    \"\"\" This function reports the performance of the provided model\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(model) == sklearn.model_selection._search.RandomizedSearchCV:\n",
    "        #Generate predictions\n",
    "        y_pred_log = model.best_estimator_.predict(X_test)\n",
    "        print(model.best_estimator_)\n",
    "    \n",
    "    elif type(model) == sklearn.model_selection._search.GridSearchCV:\n",
    "        #Generate predictions\n",
    "        y_pred_log = model.best_estimator_.predict(X_test)\n",
    "        print(model.best_estimator_)\n",
    "    \n",
    "    else:\n",
    "        #Generate predictions\n",
    "        y_pred_log = model.predict(X_test)\n",
    "        print(model)\n",
    "    \n",
    "    \n",
    "    #as the modeled was trainned using log(y) we need to revert the scale\n",
    "    y_pred = np.exp(y_pred_log)\n",
    "    \n",
    "    #Accuracy metrics for the model\n",
    "    \n",
    "    print('\\nR2 for test data : ' + str(r2_score(y_test, y_pred)))\n",
    "    print('Mean Squared Error for test data: ' + str(mean_squared_error(y_test, y_pred)) + '\\n \\n \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Running test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the created functions\n",
    "df_final = load_data('Regression_Supervised_Test_1_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the simple imputer\n",
    "import pickle\n",
    "imputer = pickle.load(open('simple_imputer.sav', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = clean_missing(df,0.8,imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lotid</th>\n",
       "      <th>aircond</th>\n",
       "      <th>numbath</th>\n",
       "      <th>numbedroom</th>\n",
       "      <th>qualitybuild</th>\n",
       "      <th>finishedarea1st</th>\n",
       "      <th>finishedarea</th>\n",
       "      <th>finishedareaEntry</th>\n",
       "      <th>countycode</th>\n",
       "      <th>numfireplace</th>\n",
       "      <th>...</th>\n",
       "      <th>regioncode</th>\n",
       "      <th>roomnum</th>\n",
       "      <th>unitnum</th>\n",
       "      <th>year</th>\n",
       "      <th>numstories</th>\n",
       "      <th>buildvalue</th>\n",
       "      <th>parcelvalue</th>\n",
       "      <th>taxyear</th>\n",
       "      <th>landvalue</th>\n",
       "      <th>totaltaxvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11614222.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1294.5</td>\n",
       "      <td>1618.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>165179.0</td>\n",
       "      <td>452044.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>286865.0</td>\n",
       "      <td>5517.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11555888.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1294.5</td>\n",
       "      <td>1817.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96047.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1948.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>295765.0</td>\n",
       "      <td>498563.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>202798.0</td>\n",
       "      <td>6188.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>17240007.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1891.0</td>\n",
       "      <td>1891.0</td>\n",
       "      <td>1891.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>97116.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>193700.0</td>\n",
       "      <td>322830.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>129130.0</td>\n",
       "      <td>3810.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13052987.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1294.5</td>\n",
       "      <td>1549.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96469.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>110977.0</td>\n",
       "      <td>336086.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>225109.0</td>\n",
       "      <td>4245.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12448473.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1294.5</td>\n",
       "      <td>1275.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96239.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>146117.0</td>\n",
       "      <td>215639.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>69522.0</td>\n",
       "      <td>2739.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5299</td>\n",
       "      <td>11103888.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1294.5</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96374.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>99641.0</td>\n",
       "      <td>346055.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>246414.0</td>\n",
       "      <td>4759.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>11614730.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1294.5</td>\n",
       "      <td>1368.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195000.0</td>\n",
       "      <td>608000.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>413000.0</td>\n",
       "      <td>7713.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5301</td>\n",
       "      <td>14195825.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1294.5</td>\n",
       "      <td>1792.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>6059.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>97079.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>182416.0</td>\n",
       "      <td>270911.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>88495.0</td>\n",
       "      <td>3672.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5302</td>\n",
       "      <td>10910905.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1294.5</td>\n",
       "      <td>1506.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96449.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>146487.0</td>\n",
       "      <td>189707.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>43220.0</td>\n",
       "      <td>2368.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5303</td>\n",
       "      <td>10729267.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1294.5</td>\n",
       "      <td>1706.0</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96389.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>274621.0</td>\n",
       "      <td>548202.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>273581.0</td>\n",
       "      <td>6759.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5304 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lotid  aircond  numbath  numbedroom  qualitybuild  finishedarea1st  \\\n",
       "0     11614222.0      1.0      3.0         3.0           4.0           1294.5   \n",
       "1     11555888.0      1.0      2.0         3.0           4.0           1294.5   \n",
       "2     17240007.0      1.0      2.0         4.0           6.0           1891.0   \n",
       "3     13052987.0      1.0      2.0         3.0           8.0           1294.5   \n",
       "4     12448473.0      1.0      3.0         3.0           4.0           1294.5   \n",
       "...          ...      ...      ...         ...           ...              ...   \n",
       "5299  11103888.0      1.0      1.0         4.0           7.0           1294.5   \n",
       "5300  11614730.0      1.0      3.0         2.0           4.0           1294.5   \n",
       "5301  14195825.0     13.0      2.5         3.0           6.0           1294.5   \n",
       "5302  10910905.0      1.0      3.0         2.0           4.0           1294.5   \n",
       "5303  10729267.0      1.0      2.0         3.0           8.0           1294.5   \n",
       "\n",
       "      finishedarea  finishedareaEntry  countycode  numfireplace  ...  \\\n",
       "0           1618.0             1297.0      6037.0           0.0  ...   \n",
       "1           1817.0             1297.0      6037.0           0.0  ...   \n",
       "2           1891.0             1891.0      6111.0           1.0  ...   \n",
       "3           1549.0             1297.0      6037.0           0.0  ...   \n",
       "4           1275.0             1297.0      6037.0           0.0  ...   \n",
       "...            ...                ...         ...           ...  ...   \n",
       "5299        1140.0             1297.0      6037.0           0.0  ...   \n",
       "5300        1368.0             1297.0      6037.0           0.0  ...   \n",
       "5301        1792.0             1297.0      6059.0           0.0  ...   \n",
       "5302        1506.0             1297.0      6037.0           0.0  ...   \n",
       "5303        1706.0             1297.0      6037.0           0.0  ...   \n",
       "\n",
       "      regioncode  roomnum  unitnum    year  numstories  buildvalue  \\\n",
       "0        96006.0      0.0      1.0  1975.0         2.0    165179.0   \n",
       "1        96047.0      0.0      1.0  1948.0         2.0    295765.0   \n",
       "2        97116.0      7.0      1.0  1976.0         1.0    193700.0   \n",
       "3        96469.0      0.0      1.0  1978.0         2.0    110977.0   \n",
       "4        96239.0      0.0      1.0  1991.0         2.0    146117.0   \n",
       "...          ...      ...      ...     ...         ...         ...   \n",
       "5299     96374.0      0.0      1.0  1962.0         2.0     99641.0   \n",
       "5300     96006.0      0.0      1.0  1981.0         2.0    195000.0   \n",
       "5301     97079.0      7.0      1.0  1984.0         2.0    182416.0   \n",
       "5302     96449.0      0.0      1.0  1973.0         2.0    146487.0   \n",
       "5303     96389.0      0.0      1.0  1956.0         2.0    274621.0   \n",
       "\n",
       "      parcelvalue  taxyear  landvalue  totaltaxvalue  \n",
       "0        452044.0   2015.0   286865.0        5517.93  \n",
       "1        498563.0   2015.0   202798.0        6188.97  \n",
       "2        322830.0   2015.0   129130.0        3810.64  \n",
       "3        336086.0   2016.0   225109.0        4245.09  \n",
       "4        215639.0   2015.0    69522.0        2739.32  \n",
       "...           ...      ...        ...            ...  \n",
       "5299     346055.0   2015.0   246414.0        4759.14  \n",
       "5300     608000.0   2015.0   413000.0        7713.21  \n",
       "5301     270911.0   2015.0    88495.0        3672.92  \n",
       "5302     189707.0   2015.0    43220.0        2368.38  \n",
       "5303     548202.0   2016.0   273581.0        6759.65  \n",
       "\n",
       "[5304 rows x 31 columns]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: the variable  mypointer was not found in the data provided.\n"
     ]
    }
   ],
   "source": [
    "X_final_test, y_final_test = separate_features(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column  aircond_3.0  added to the dataset.\n",
      "Column  heatingtype_11.0  added to the dataset.\n",
      "Column  heatingtype_12.0  added to the dataset.\n",
      "Column  num34bath  added to the dataset.\n"
     ]
    }
   ],
   "source": [
    "X_final_test = feature_crafting(X_final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5304, 54)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the optimal models from the files\n",
    "import pickle\n",
    "forest_opt = pickle.load(open('tuned_forest.sav', 'rb')) \n",
    "xgb_opt = pickle.load(open('tuned_xgb.sav', 'rb')) \n",
    "ada_opt = pickle.load(open('tuned_ada.sav', 'rb'))\n",
    "gb_opt = pickle.load(open('tuned_gb.sav', 'rb'))\n",
    "ensemble_opt = pickle.load(open('tuned_ensemble.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=117,\n",
      "                      max_features='auto', max_leaf_nodes=137,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=13, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=71,\n",
      "                      n_jobs=None, oob_score=False, random_state=None,\n",
      "                      verbose=0, warm_start=False)\n",
      "\n",
      "R2 for test data : 0.4982536366504079\n",
      "Mean Squared Error for test data: 237628796665.66696\n",
      " \n",
      " \n",
      "\n",
      "AdaBoostRegressor(base_estimator=None, learning_rate=2.5, loss='linear',\n",
      "                  n_estimators=50, random_state=92)\n",
      "\n",
      "R2 for test data : 0.4126264879971111\n",
      "Mean Squared Error for test data: 278182107626.52386\n",
      " \n",
      " \n",
      "\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7, gamma=0.075,\n",
      "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
      "             max_depth=8, min_child_weight=7, missing=nan, n_estimators=100,\n",
      "             n_jobs=1, nthread=None, objective='reg:squarederror',\n",
      "             random_state=92, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             seed=None, silent=None, subsample=0.8, tree_method='auto',\n",
      "             verbosity=1)\n",
      "\n",
      "R2 for test data : 0.47742676202751844\n",
      "Mean Squared Error for test data: 247492475839.9506\n",
      " \n",
      " \n",
      "\n",
      "GradientBoostingRegressor(alpha=0.8, criterion='friedman_mse', init=None,\n",
      "                          learning_rate=0.1, loss='ls', max_depth=6,\n",
      "                          max_features=None, max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=1, min_samples_split=2,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=80,\n",
      "                          n_iter_no_change=None, presort='auto',\n",
      "                          random_state=92, subsample=0.8, tol=0.0001,\n",
      "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "R2 for test data : 0.4618796853179884\n",
      "Mean Squared Error for test data: 254855624633.8769\n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [forest_opt, ada_opt, xgb_opt, gb_opt]\n",
    "\n",
    "for model in models:\n",
    "    model_performance(model, X_final_test, y_final_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "SVM.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "418.9px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": null,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
