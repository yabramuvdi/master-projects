{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>positive</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>41994</td>\n",
       "      <td>SycRVheLdFcK-2jt8j5d4Q</td>\n",
       "      <td>False</td>\n",
       "      <td>If the walls of this place could speak they wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41774</td>\n",
       "      <td>SvsAj_yb9kGOYjknjrc45A</td>\n",
       "      <td>False</td>\n",
       "      <td>I went in with a rash on my hand. They informe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49038</td>\n",
       "      <td>kRhjWeAPs-U5RmakIKz0Pg</td>\n",
       "      <td>True</td>\n",
       "      <td>You're out on a Saturday night and you're not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97374</td>\n",
       "      <td>l07ctcrDMV--TYwe3uzebQ</td>\n",
       "      <td>True</td>\n",
       "      <td>Whenever I feel like going \"tanning\" (that's m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16344</td>\n",
       "      <td>iE71iwcSljg3xm2GB2Y9aA</td>\n",
       "      <td>True</td>\n",
       "      <td>This is my favorite restaurant. Not only becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1055</td>\n",
       "      <td>cCFWegvTavai-oOE4C4sDA</td>\n",
       "      <td>False</td>\n",
       "      <td>Make that minus one star. Even worse than the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60470</td>\n",
       "      <td>XXW_OFaYQkkGOGniujZFHg</td>\n",
       "      <td>False</td>\n",
       "      <td>This place suckkkkedddd. Bad food. \\nI came he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6390</td>\n",
       "      <td>LPMZ9N1sAjs2nDx7DmiZ2w</td>\n",
       "      <td>False</td>\n",
       "      <td>Tasteless pizza, nasty sauce, and didn't liste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48240</td>\n",
       "      <td>oiVpyFXOAdQUHvQLwr-58g</td>\n",
       "      <td>True</td>\n",
       "      <td>Its was the 4th of July and hot as heck. I was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79361</td>\n",
       "      <td>hTeEouiPuF8QhcKRm-YsjA</td>\n",
       "      <td>True</td>\n",
       "      <td>I'm soo happy with my visit to The Joint! It i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  business_id positive  \\\n",
       "41994  SycRVheLdFcK-2jt8j5d4Q    False   \n",
       "41774  SvsAj_yb9kGOYjknjrc45A    False   \n",
       "49038  kRhjWeAPs-U5RmakIKz0Pg     True   \n",
       "97374  l07ctcrDMV--TYwe3uzebQ     True   \n",
       "16344  iE71iwcSljg3xm2GB2Y9aA     True   \n",
       "...                       ...      ...   \n",
       "1055   cCFWegvTavai-oOE4C4sDA    False   \n",
       "60470  XXW_OFaYQkkGOGniujZFHg    False   \n",
       "6390   LPMZ9N1sAjs2nDx7DmiZ2w    False   \n",
       "48240  oiVpyFXOAdQUHvQLwr-58g     True   \n",
       "79361  hTeEouiPuF8QhcKRm-YsjA     True   \n",
       "\n",
       "                                                    text  \n",
       "41994  If the walls of this place could speak they wo...  \n",
       "41774  I went in with a rash on my hand. They informe...  \n",
       "49038  You're out on a Saturday night and you're not ...  \n",
       "97374  Whenever I feel like going \"tanning\" (that's m...  \n",
       "16344  This is my favorite restaurant. Not only becau...  \n",
       "...                                                  ...  \n",
       "1055   Make that minus one star. Even worse than the ...  \n",
       "60470  This place suckkkkedddd. Bad food. \\nI came he...  \n",
       "6390   Tasteless pizza, nasty sauce, and didn't liste...  \n",
       "48240  Its was the 4th of July and hot as heck. I was...  \n",
       "79361  I'm soo happy with my visit to The Joint! It i...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp = pd.read_csv(\"sentiment/yelps.csv\")\n",
    "# I need to work with a fraction of the data (or else my computer crashes)\n",
    "yelp = yelp.sample(frac=0.1)\n",
    "yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>positive</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20944</td>\n",
       "      <td>11475</td>\n",
       "      <td>False</td>\n",
       "      <td>A movie that makes you want to throw yourself ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16558</td>\n",
       "      <td>8392</td>\n",
       "      <td>True</td>\n",
       "      <td>This film exhibits artful cinematic techniques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>11095</td>\n",
       "      <td>True</td>\n",
       "      <td>This movie is based on the true story of Chris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9246</td>\n",
       "      <td>8555</td>\n",
       "      <td>False</td>\n",
       "      <td>This is a very old and cheaply made film--a ty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49910</td>\n",
       "      <td>1520</td>\n",
       "      <td>False</td>\n",
       "      <td>Being a HUGE fan of the bottom series i was re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41671</td>\n",
       "      <td>9983</td>\n",
       "      <td>True</td>\n",
       "      <td>This movie is about basically human relations,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27120</td>\n",
       "      <td>3610</td>\n",
       "      <td>True</td>\n",
       "      <td>In my opinion, this film has wonderful lightin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40395</td>\n",
       "      <td>1120</td>\n",
       "      <td>False</td>\n",
       "      <td>I saw this movie in my international cinema cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47449</td>\n",
       "      <td>9636</td>\n",
       "      <td>True</td>\n",
       "      <td>This is another classic Seagal movie. He walks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25283</td>\n",
       "      <td>9803</td>\n",
       "      <td>True</td>\n",
       "      <td>This Batman movie isn't quite as good as Batma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10001 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       movie positive                                               text\n",
       "20944  11475    False  A movie that makes you want to throw yourself ...\n",
       "16558   8392     True  This film exhibits artful cinematic techniques...\n",
       "310    11095     True  This movie is based on the true story of Chris...\n",
       "9246    8555    False  This is a very old and cheaply made film--a ty...\n",
       "49910   1520    False  Being a HUGE fan of the bottom series i was re...\n",
       "...      ...      ...                                                ...\n",
       "41671   9983     True  This movie is about basically human relations,...\n",
       "27120   3610     True  In my opinion, this film has wonderful lightin...\n",
       "40395   1120    False  I saw this movie in my international cinema cl...\n",
       "47449   9636     True  This is another classic Seagal movie. He walks...\n",
       "25283   9803     True  This Batman movie isn't quite as good as Batma...\n",
       "\n",
       "[10001 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb = pd.read_csv(\"sentiment/movies.csv\")\n",
    "imdb = imdb.sample(frac=0.2)\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_alphanumeric_or_space = re.compile('[^(\\w|\\s|\\d)]')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess(doc):\n",
    "    doc = re.sub(not_alphanumeric_or_space, '', doc)\n",
    "    words = [t.lemma_ for t in nlp(doc) if t.lemma_ != '-PRON-']\n",
    "    return ' '.join(words).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_yelp = TfidfVectorizer(min_df=2,\n",
    "                         max_df=.8,\n",
    "                         preprocessor=preprocess,\n",
    "                         stop_words='english',\n",
    "                         use_idf=True,\n",
    "                         norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yabra/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# fit the vectorizer to the Yelps dataset and get the features\n",
    "features_yelp = vect_yelp.fit_transform(yelp.text)\n",
    "features_yelp = features_yelp.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this same vectorizer to transform the IMDB dataset\n",
    "features_imdb = vect_yelp.transform(imdb.text)\n",
    "features_imdb = features_imdb.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Fitting the models\n",
    "\n",
    "Now that we have vectorized our data we can fit different models in order to predict the sentiment of each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the two basic models we will use (discriminative and generative approach)\n",
    "NB = MultinomialNB(fit_prior=False)\n",
    "LR = LogisticRegression(solver='lbfgs', n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the logistic regression\n",
    "y_yelp = yelp.positive\n",
    "fitted_LR = LR.fit(features_yelp, y_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93041392, 0.9289858 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets get a first assessment of the accuracy of the model (for the Yelp data)\n",
    "cross_val_score(fitted_LR, features_yelp, y_yelp, cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the Naive Bayes\n",
    "fitted_NB = NB.fit(features_yelp, y_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86082783, 0.87357471])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get accuracy assesment (for the Yelp data as well)\n",
    "cross_val_score(fitted_NB, features_yelp, y_yelp, cv = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Transfer learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the target variable for the movies dataset\n",
    "y_imdb = imdb.positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for both models using the fitted models!\n",
    "y_NB = fitted_NB.predict(features_imdb)\n",
    "y_LR = fitted_LR.predict(features_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the results\n",
    "results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'direct_transfer': (0.7011298870112989, 0.7381261873812619)}\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy and store it\n",
    "results['direct_transfer'] = accuracy_score(y_NB, y_imdb), accuracy_score(y_LR, y_imdb) \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we are not performing as well as we did in the Yelp data... There are some problems when transfering what we learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosstraining 2\n",
    "\n",
    "Try to improve your transfer score using the unlabelled target data, P(X). What could you learn from the target context (without using the labels, only from the X) that might help you when training your model on your source context? How can you construct a feature space in your source context that generalizes better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea 1: Use only words that are common to both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ******* Number of common words in both datasets: 27666\n"
     ]
    }
   ],
   "source": [
    "yelp_words = [word.lower() for doc in yelp.text for word in doc.split()]\n",
    "yelp_words = set(yelp_words)\n",
    "\n",
    "imdb_words = [word.lower() for doc in imdb.text for word in doc.split()]\n",
    "imdb_words = set(imdb_words)\n",
    "\n",
    "common_words = yelp_words.intersection(imdb_words)\n",
    "common_words\n",
    "print(' ******* Number of common words in both datasets:', len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_alphanumeric_or_space = re.compile('[^(\\w|\\s|\\d)]')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess(doc):\n",
    "    doc = re.sub(not_alphanumeric_or_space, '', doc.lower())\n",
    "    words = [t for t in doc if t in common_words]\n",
    "    words = [t.lemma_ for t in nlp(doc) if t.lemma_ != '-PRON-']\n",
    "    return ' '.join(words).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_yelp = TfidfVectorizer(min_df=2,\n",
    "                         max_df=.8,\n",
    "                         preprocessor=preprocess,\n",
    "                         stop_words='english',\n",
    "                         use_idf=True,\n",
    "                         norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yabra/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# fit the vectorizer to the Yelps dataset and get the features\n",
    "features_yelp = vect_yelp.fit_transform(yelp.text)\n",
    "features_yelp = features_yelp.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this same vectorizer to transform the IMDB dataset\n",
    "features_imdb = vect_yelp.transform(imdb.text)\n",
    "features_imdb = features_imdb.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the two basic models we will use (discriminative and generative approach)\n",
    "NB = MultinomialNB(fit_prior=False)\n",
    "LR = LogisticRegression(solver='lbfgs', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the logistic regression\n",
    "fitted_LR = LR.fit(features_yelp, y_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the Naive Bayes\n",
    "fitted_NB = NB.fit(features_yelp, y_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for both models using the fitted models!\n",
    "y_NB = fitted_NB.predict(features_imdb)\n",
    "y_LR = fitted_LR.predict(features_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'direct_transfer': (0.7011298870112989, 0.7381261873812619), 'common_words': (0.6978302169783022, 0.7398260173982601)}\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy and save it\n",
    "results['common_words'] = accuracy_score(y_NB, y_imdb), accuracy_score(y_LR, y_imdb) \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea 2: Restrict the vocabulary to the words with the highes IDF in the target space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate this class just to get the IDF in the target space\n",
    "vect_imdb = TfidfVectorizer(min_df=2,\n",
    "                         max_df=.8,\n",
    "                         preprocessor=preprocess,\n",
    "                         stop_words='english',\n",
    "                         use_idf=True,\n",
    "                         norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yabra/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9.11192806, 9.11192806, 8.13109881, ..., 9.11192806, 9.11192806,\n",
       "       8.82424599])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_imdb.fit(imdb.text)\n",
    "vect_imdb.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28093"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = np.flip(np.argsort(vect_imdb.idf_))\n",
    "len(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select words with the highest IDF (25%)\n",
    "idf_top = idf[0:int(len(idf)*0.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = []\n",
    "for word, idx in vect_imdb.vocabulary_.items():\n",
    "    if idx in idf_top:\n",
    "        top_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_alphanumeric_or_space = re.compile('[^(\\w|\\s|\\d)]')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess(doc):\n",
    "    doc = re.sub(not_alphanumeric_or_space, '', doc.lower())\n",
    "    words = [t for t in doc if t in top_words]\n",
    "    words = [t.lemma_ for t in nlp(doc) if t.lemma_ != '-PRON-']\n",
    "    return ' '.join(words).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_yelp = TfidfVectorizer(min_df=2,\n",
    "                         max_df=.8,\n",
    "                         preprocessor=preprocess,\n",
    "                         stop_words='english',\n",
    "                         use_idf=True,\n",
    "                         norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yabra/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# fit the vectorizer to the Yelps dataset and get the features\n",
    "features_yelp = vect_yelp.fit_transform(yelp.text)\n",
    "features_yelp = features_yelp.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this same vectorizer to transform the IMDB dataset\n",
    "features_imdb = vect_yelp.transform(imdb.text)\n",
    "features_imdb = features_imdb.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the two basic models we will use (discriminative and generative approach)\n",
    "NB = MultinomialNB(fit_prior=False)\n",
    "LR = LogisticRegression(solver='lbfgs', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the logistic regression\n",
    "fitted_LR = LR.fit(features_yelp, y_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the Naive Bayes\n",
    "fitted_NB = NB.fit(features_yelp, y_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for both models using the fitted models!\n",
    "y_NB = fitted_NB.predict(features_imdb)\n",
    "y_LR = fitted_LR.predict(features_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'direct_transfer': (0.7011298870112989, 0.7381261873812619), 'common_words': (0.6978302169783022, 0.7398260173982601), 'top_idf_vocab': (0.6978302169783022, 0.7398260173982601)}\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy and save it\n",
    "results['top_idf_vocab'] = accuracy_score(y_NB, y_imdb), accuracy_score(y_LR, y_imdb) \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea 3: PCA Sample selection - [Xi et al. (2015)](https://www.sentic.net/domain-adaptation-for-sentiment-classification.pdf)\n",
    "\n",
    "1. PCA on target X\n",
    "2. Identify documents in source with lower distance (Hotelling T to PCA)\n",
    "3. Use only those documents (thus the sampling part) to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_alphanumeric_or_space = re.compile('[^(\\w|\\s|\\d)]')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess(doc):\n",
    "    doc = re.sub(not_alphanumeric_or_space, '', doc)\n",
    "    words = [t.lemma_ for t in nlp(doc) if t.lemma_ != '-PRON-']\n",
    "    return ' '.join(words).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_yelp = TfidfVectorizer(min_df=2,\n",
    "                         max_df=.8,\n",
    "                         preprocessor=preprocess,\n",
    "                         stop_words='english',\n",
    "                         use_idf=True,\n",
    "                         norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yabra/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# fit the vectorizer to the Yelps dataset and get the features\n",
    "features_yelp = vect_yelp.fit_transform(yelp.text)\n",
    "features_yelp = features_yelp.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this same vectorizer to transform the IMDB dataset\n",
    "features_imdb = vect_yelp.transform(imdb.text)\n",
    "features_imdb = features_imdb.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=50, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "pca.fit(features_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings_p = pca.components_.T\n",
    "eigenvalues = pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotelling_t2s(loadings_p, eigenvalues, xi): \n",
    "    dist = np.array(xi.dot(loadings_p)\n",
    "                            .dot(np.diag(eigenvalues ** -1))\n",
    "                            .dot(loadings_p.T)\n",
    "                            .dot(xi.T))\n",
    "    return(dist[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_hotelling(loadings_p, eigenvalues, X, y, best):\n",
    "    all_dist = []\n",
    "    for xi in X:\n",
    "        all_dist.append(hotelling_t2s(loadings_p,eigenvalues,xi))\n",
    "    \n",
    "    indices = list(np.flip(np.argsort(all_dist)))[0:best]\n",
    "    sub_X = X[indices]\n",
    "    sub_y = np.array(y)[indices]\n",
    "    \n",
    "    return(sub_X, sub_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub_yelp, y_sub_yelp = high_hotelling(loadings_p, eigenvalues, features_yelp, y_yelp, int(len(y_yelp)*0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the two basic models we will use (discriminative and generative approach)\n",
    "NB = MultinomialNB(fit_prior=False)\n",
    "LR = LogisticRegression(solver='lbfgs', n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the logistic regression\n",
    "fitted_LR = LR.fit(X_sub_yelp, y_sub_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the Naive Bayes\n",
    "fitted_NB = NB.fit(X_sub_yelp, y_sub_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for both models using the fitted models!\n",
    "y_NB = fitted_NB.predict(features_imdb)\n",
    "y_LR = fitted_LR.predict(features_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'direct_transfer': (0.7011298870112989, 0.7381261873812619), 'common_words': (0.6978302169783022, 0.7398260173982601), 'top_idf_vocab': (0.6978302169783022, 0.7398260173982601), 'PCA_SS': (0.6446355364463554, 0.7077292270772922)}\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy and save it\n",
    "results['PCA_SS'] = accuracy_score(y_NB, y_imdb), accuracy_score(y_LR, y_imdb) \n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "assignment.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
